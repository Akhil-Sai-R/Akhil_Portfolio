[{"content":"Description This Web-Based CNN Model Tester and Trainer is an intuitive application designed to simplify AI model development for image-based tasks like object detection, classification, and recognition. The platform enables users to create, train, and test AI models without requiring expertise in programming or machine learning.\nKey Features Automated Data Preprocessing: Handles cleaning, augmentation, and preparation of image datasets. End-to-End Model Training: Trains Convolutional Neural Network (CNN) models using uploaded datasets. Model Testing: Allows users to upload images or capture them in real time for testing model predictions. Model Deployment: Provides the trained model in formats like H5, JSON, ready for deployment on edge devices. Real-Time Dataset Capture: Supports capturing datasets directly via webcam or connected cameras. User-Friendly Workflow: Offers a no-code experience for AI enthusiasts, researchers, and developers. Working Process Data Input: Users can upload datasets or capture real-time images for training and testing. Data Preprocessing: The tool cleans and prepares the data automatically, ensuring high-quality inputs. Model Training: CNN models are trained using TensorFlow, OpenVINO, or ONNX. Model Testing: Users can test the model on uploaded images or real-time captures. Model Export: Trained models are made available in formats such as H5, JSON, and ONNX for deployment. Applications Autonomous Vehicle Development: Used by AVL Software and Functions GmbH for ADAS (Advanced Driver Assistance Systems) and autonomous vehicle projects. AI Learning and Research: Makes AI accessible for researchers and enthusiasts by eliminating the need for advanced programming skills. Custom AI Solutions: Suitable for creating tailor-made AI models for object detection and recognition. Tech Stack Programming Languages: Python, HTML, CSS, JavaScript. Frameworks and Libraries: TensorFlow, Keras, OpenCV, TensorflowJS, Flask, FAST API. Tools and Technologies: Protobuf, OpenVINO, ONNX, TensorBoard, CUDA, cuDNN. Neural Network: Convolutional Neural Network (CNN). Impact The primary aim of this project is to make AI model development easy, quick, and accessible, empowering users with limited technical knowledge to develop AI solutions independently. The platform’s deployment at AVL Software and Functions GmbH demonstrates its effectiveness in supporting ADAS and autonomous vehicle projects.\n","permalink":"http://localhost:1313/Akhil_Portfolio/projects/master_thesis/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThis \u003cstrong\u003eWeb-Based CNN Model Tester and Trainer\u003c/strong\u003e is an intuitive application designed to simplify AI model development for image-based tasks like \u003cstrong\u003eobject detection, classification, and recognition\u003c/strong\u003e. The platform enables users to create, train, and test AI models without requiring expertise in programming or machine learning.\u003c/p\u003e\n\u003ch3 id=\"key-features\"\u003e\u003cstrong\u003eKey Features\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eAutomated Data Preprocessing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eHandles cleaning, augmentation, and preparation of image datasets.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEnd-to-End Model Training:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eTrains Convolutional Neural Network (CNN) models using uploaded datasets.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel Testing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eAllows users to upload images or capture them in real time for testing model predictions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel Deployment:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eProvides the trained model in formats like \u003cstrong\u003eH5, JSON\u003c/strong\u003e, ready for deployment on edge devices.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReal-Time Dataset Capture:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eSupports capturing datasets directly via webcam or connected cameras.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUser-Friendly Workflow:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eOffers a no-code experience for AI enthusiasts, researchers, and developers.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"working-process\"\u003e\u003cstrong\u003eWorking Process\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Input:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUsers can upload datasets or capture real-time images for training and testing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Preprocessing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eThe tool cleans and prepares the data automatically, ensuring high-quality inputs.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel Training:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eCNN models are trained using TensorFlow, OpenVINO, or ONNX.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel Testing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUsers can test the model on uploaded images or real-time captures.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel Export:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eTrained models are made available in formats such as H5, JSON, and ONNX for deployment.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"applications\"\u003e\u003cstrong\u003eApplications\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAutonomous Vehicle Development:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUsed by AVL Software and Functions GmbH for ADAS (Advanced Driver Assistance Systems) and autonomous vehicle projects.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAI Learning and Research:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eMakes AI accessible for researchers and enthusiasts by eliminating the need for advanced programming skills.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCustom AI Solutions:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eSuitable for creating tailor-made AI models for object detection and recognition.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"tech-stack\"\u003e\u003cstrong\u003eTech Stack\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProgramming Languages:\u003c/strong\u003e Python, HTML, CSS, JavaScript.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFrameworks and Libraries:\u003c/strong\u003e TensorFlow, Keras, OpenCV, TensorflowJS, Flask, FAST API.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTools and Technologies:\u003c/strong\u003e Protobuf, OpenVINO, ONNX, TensorBoard, CUDA, cuDNN.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNeural Network:\u003c/strong\u003e Convolutional Neural Network (CNN).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"impact\"\u003e\u003cstrong\u003eImpact\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eThe primary aim of this project is to make AI model development \u003cstrong\u003eeasy, quick, and accessible\u003c/strong\u003e, empowering users with limited technical knowledge to develop AI solutions independently. The platform’s deployment at AVL Software and Functions GmbH demonstrates its effectiveness in supporting \u003cstrong\u003eADAS and autonomous vehicle projects\u003c/strong\u003e.\u003c/p\u003e","title":"Web-Based CNN Model Tester and Trainer"},{"content":"Description Project MOWME is an innovative Autonomous Driving Rover prototype designed to showcase advanced AI, ML, and IoT-driven capabilities. The rover is built to autonomously navigate, classify objects, collect and self-train data, and transfer insights to the cloud for future learning. This system integrates advanced hardware with cloud storage and machine learning for real-time decision-making and environmental analysis.\nKey Features Object Classification: The rover uses machine learning to identify and classify objects in its environment. Auto-Data Collection: Captures real-time sensor and camera data, processing it locally or storing it in the cloud. Self-Training: Implements feedback loops to refine object classification and navigation algorithms over time. Auto-Navigation: Incorporates GPS waypoint navigation for autonomous movement across dynamic terrains. Obstacle Avoidance: Uses ultrasonic sensors for collision detection and avoidance. Cloud Integration: Data is stored on AWS Cloud, enabling future training and analysis. Environmental Monitoring: Detects diseases in grass using Convolutional Neural Networks (CNN), identifying patterns like dryness or overwatering. Hardware Components Core Components: Raspberry Pi 4 Arduino Raspberry Pi Camera L298D Motor Drive Additional Sensors: Ultrasonic Sensors GPS Module Bluetooth Module Software Features Wireless Drive Control: The rover can be controlled using MQTT protocol for wireless operation. Video Streaming: Real-time video streaming from Raspberry Pi to a laptop for monitoring. Dataset Collection: Automatically captures and processes images for model training, storing them locally or in the cloud. Disease Detection in Grass: Leverages CNN models to analyze grass conditions and classify diseases. Working Process Data Collection: Captures sensor data, camera images, and ultrasonic measurements. Transfers data to AWS Cloud for processing and storage. Data Processing: Object classification and dataset analysis using TensorFlow and CNN models. Disease detection in grass based on visual patterns. Navigation and Control: Utilizes GPS and sensor data for waypoint navigation. Avoids obstacles dynamically with ultrasonic sensors. Wireless Communication: MQTT protocol enables seamless communication between devices. Live monitoring of rover operations via Raspberry Pi. Programming Languages Python: (80%) Core logic, machine learning models, and data processing. C++: (20%) For low-level hardware interaction and control. AWS Integration: IoT and cloud-based storage for scalability. Applications Autonomous lawn management systems. Environmental monitoring and data collection. IoT-driven robotics for agriculture. AI-based disease detection and analytics. The MOWME Rover demonstrates the seamless integration of hardware, AI, and cloud services, offering a versatile solution for autonomous systems and environmental analysis. This project is a step forward in creating smart, scalable, and adaptive IoT-enabled devices for practical use cases.\n","permalink":"http://localhost:1313/Akhil_Portfolio/projects/mowme/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eProject \u003cstrong\u003eMOWME\u003c/strong\u003e is an innovative \u003cstrong\u003eAutonomous Driving Rover\u003c/strong\u003e prototype designed to showcase advanced AI, ML, and IoT-driven capabilities. The rover is built to autonomously navigate, classify objects, collect and self-train data, and transfer insights to the cloud for future learning. This system integrates advanced hardware with cloud storage and machine learning for real-time decision-making and environmental analysis.\u003c/p\u003e\n\u003ch3 id=\"key-features\"\u003e\u003cstrong\u003eKey Features\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eObject Classification:\u003c/strong\u003e The rover uses machine learning to identify and classify objects in its environment.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAuto-Data Collection:\u003c/strong\u003e Captures real-time sensor and camera data, processing it locally or storing it in the cloud.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Training:\u003c/strong\u003e Implements feedback loops to refine object classification and navigation algorithms over time.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAuto-Navigation:\u003c/strong\u003e Incorporates \u003cstrong\u003eGPS waypoint navigation\u003c/strong\u003e for autonomous movement across dynamic terrains.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eObstacle Avoidance:\u003c/strong\u003e Uses ultrasonic sensors for collision detection and avoidance.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCloud Integration:\u003c/strong\u003e Data is stored on \u003cstrong\u003eAWS Cloud\u003c/strong\u003e, enabling future training and analysis.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEnvironmental Monitoring:\u003c/strong\u003e Detects diseases in grass using \u003cstrong\u003eConvolutional Neural Networks (CNN)\u003c/strong\u003e, identifying patterns like dryness or overwatering.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"hardware-components\"\u003e\u003cstrong\u003eHardware Components\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCore Components:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eRaspberry Pi 4\u003c/li\u003e\n\u003cli\u003eArduino\u003c/li\u003e\n\u003cli\u003eRaspberry Pi Camera\u003c/li\u003e\n\u003cli\u003eL298D Motor Drive\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAdditional Sensors:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUltrasonic Sensors\u003c/li\u003e\n\u003cli\u003eGPS Module\u003c/li\u003e\n\u003cli\u003eBluetooth Module\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"software-features\"\u003e\u003cstrong\u003eSoftware Features\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eWireless Drive Control:\u003c/strong\u003e The rover can be controlled using MQTT protocol for wireless operation.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVideo Streaming:\u003c/strong\u003e Real-time video streaming from Raspberry Pi to a laptop for monitoring.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDataset Collection:\u003c/strong\u003e Automatically captures and processes images for model training, storing them locally or in the cloud.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDisease Detection in Grass:\u003c/strong\u003e Leverages CNN models to analyze grass conditions and classify diseases.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"working-process\"\u003e\u003cstrong\u003eWorking Process\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Collection:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eCaptures sensor data, camera images, and ultrasonic measurements.\u003c/li\u003e\n\u003cli\u003eTransfers data to AWS Cloud for processing and storage.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Processing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eObject classification and dataset analysis using \u003cstrong\u003eTensorFlow\u003c/strong\u003e and \u003cstrong\u003eCNN models\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eDisease detection in grass based on visual patterns.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNavigation and Control:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUtilizes GPS and sensor data for waypoint navigation.\u003c/li\u003e\n\u003cli\u003eAvoids obstacles dynamically with ultrasonic sensors.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWireless Communication:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eMQTT protocol enables seamless communication between devices.\u003c/li\u003e\n\u003cli\u003eLive monitoring of rover operations via Raspberry Pi.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"programming-languages\"\u003e\u003cstrong\u003eProgramming Languages\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePython:\u003c/strong\u003e (80%) Core logic, machine learning models, and data processing.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eC++:\u003c/strong\u003e (20%) For low-level hardware interaction and control.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAWS Integration:\u003c/strong\u003e IoT and cloud-based storage for scalability.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"applications\"\u003e\u003cstrong\u003eApplications\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAutonomous lawn management systems.\u003c/li\u003e\n\u003cli\u003eEnvironmental monitoring and data collection.\u003c/li\u003e\n\u003cli\u003eIoT-driven robotics for agriculture.\u003c/li\u003e\n\u003cli\u003eAI-based disease detection and analytics.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003cstrong\u003eMOWME Rover\u003c/strong\u003e demonstrates the seamless integration of hardware, AI, and cloud services, offering a versatile solution for autonomous systems and environmental analysis. This project is a step forward in creating smart, scalable, and adaptive IoT-enabled devices for practical use cases.\u003c/p\u003e","title":"Autonomous Driving Rover: Project MOWME"},{"content":"Description This project focuses on developing an intelligent Assets and Alerts Copilot system that leverages graph-based data modeling and AI-powered insights to provide a comprehensive cybersecurity solution. The system integrates advanced tools like Neo4j, Mistral, and Streamlit to enable real-time asset and alert detection, relationship mapping, and contextual analysis.\nThe graph-based data modeling ensures accurate visualization of assets, products, vulnerabilities, and their relationships, providing unparalleled clarity in understanding interconnected cybersecurity risks. With the Mistral LLM, the copilot allows users to interact using natural language queries, transforming complex data into actionable insights.\nThe system provides real-time responses to queries, dynamic alert prioritization, and customizable visualizations of vulnerabilities and asset connections. It is particularly valuable for security teams seeking to reduce alert fatigue, understand critical relationships, and prioritize actionable insights.\nThis project represents a cutting-edge approach to cybersecurity operations, combining advanced AI with graph-driven analysis to enhance threat detection, incident response, and asset management.\nTechnologies Used Programming Languages: Python Libraries and Frameworks: GCP Functions, Neo4J, Streamlit Applications: IT security team ","permalink":"http://localhost:1313/Akhil_Portfolio/projects/assets-and-alerts/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThis project focuses on developing an \u003cstrong\u003eintelligent Assets and Alerts Copilot system\u003c/strong\u003e that leverages graph-based data modeling and AI-powered insights to provide a comprehensive cybersecurity solution. The system integrates advanced tools like \u003cstrong\u003eNeo4j\u003c/strong\u003e, \u003cstrong\u003eMistral\u003c/strong\u003e, and \u003cstrong\u003eStreamlit\u003c/strong\u003e to enable real-time asset and alert detection, relationship mapping, and contextual analysis.\u003c/p\u003e\n\u003cp\u003eThe \u003cstrong\u003egraph-based data modeling\u003c/strong\u003e ensures accurate visualization of assets, products, vulnerabilities, and their relationships, providing unparalleled clarity in understanding interconnected cybersecurity risks. With the \u003cstrong\u003eMistral LLM\u003c/strong\u003e, the copilot allows users to interact using natural language queries, transforming complex data into actionable insights.\u003c/p\u003e","title":"Assets and Alerts Copilot Using Graph-Based AI Systems"},{"content":"Description This project showcases the development of an Autonomous Vehicle System (AVS) prototype, integrating advanced AI, Machine Learning (ML), and Deep Learning (DL) techniques with embedded systems. The project leverages a combination of Raspberry Pi 4, Arduino, and NVIDIA Jetson Nano/TX2 NX, along with additional hardware components, to create a scalable and efficient system that simulates real-world autonomous vehicle capabilities.\nKey Features: Self-driving: Autonomous navigation with path following. Obstacle avoidance: Detection and navigation around obstacles using ultrasonic sensors and LiDAR/Radar. Self-parking: Automated parking into predefined spaces. Traffic signboard recognition: Classification of traffic signs using machine learning models. Voice command recognition: Hands-free vehicle control through voice inputs. Waypoint navigation: GPS-based precise navigation to specific destinations. Real-time data collection: Integration of telemetry and sensor data for system optimization. Hardware Components: Primary Hardware: Raspberry Pi 4 Arduino Raspberry Pi Camera with Night Vision Sensors L298D Motor Drive Raspberry Pi Microphone and Speaker Additional Hardware: Ultrasonic Sensors LiDAR/Radar GPS Module Bluetooth Module Concepts and Technologies: Computer Vision: Object detection, sign recognition, and real-time image processing using OpenCV. Machine Learning and Deep Learning: TensorFlow and PyTorch-based models for sign classification and voice recognition. GPS Navigation: Waypoint-based navigation for precise route planning. Obstacle Avoidance: Real-time distance measurement and navigation using ultrasonic sensors and LiDAR. Auto Parking: Automated parking algorithms for efficient maneuvering. Traffic Signboard Recognition: AI-driven recognition of traffic signs to simulate real-world scenarios. Programming Languages: Python: (80%) Used for most AI, ML, and sensor data processing tasks. C++: (20%) For low-level hardware integration and control. Java and CSS: For GPS tracking through mobile and smartwatch applications Working Process: Data Input: Real-time data collection from ultrasonic sensors, LiDAR, and cameras. Data Processing: Object detection and obstacle avoidance using OpenCV and machine learning models. Traffic sign recognition and decision-making with TensorFlow 2.0. Vehicle Control: Autonomous navigation and parking using processed sensor data. Output and Monitoring: Real-time visualization and monitoring via a local server (localhost). Applications: Autonomous driving research and development. Robotics and intelligent automation. Smart transportation systems. AI-based real-time monitoring and data collection. The Autonomous Driving Prototype combines cutting-edge AI techniques with practical embedded systems to offer a versatile and scalable solution for autonomous navigation and control. This project demonstrates how emerging technologies can be used to bridge the gap between AI research and real-world applications.\n","permalink":"http://localhost:1313/Akhil_Portfolio/projects/autonomus_vehicle/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThis project showcases the development of an \u003cstrong\u003eAutonomous Vehicle System (AVS) prototype\u003c/strong\u003e, integrating advanced \u003cstrong\u003eAI, Machine Learning (ML), and Deep Learning (DL)\u003c/strong\u003e techniques with embedded systems. The project leverages a combination of \u003cstrong\u003eRaspberry Pi 4\u003c/strong\u003e, \u003cstrong\u003eArduino\u003c/strong\u003e, and \u003cstrong\u003eNVIDIA Jetson Nano/TX2 NX\u003c/strong\u003e, along with additional hardware components, to create a scalable and efficient system that simulates real-world autonomous vehicle capabilities.\u003c/p\u003e\n\u003ch3 id=\"key-features\"\u003e\u003cstrong\u003eKey Features:\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-driving:\u003c/strong\u003e Autonomous navigation with path following.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eObstacle avoidance:\u003c/strong\u003e Detection and navigation around obstacles using ultrasonic sensors and LiDAR/Radar.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-parking:\u003c/strong\u003e Automated parking into predefined spaces.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraffic signboard recognition:\u003c/strong\u003e Classification of traffic signs using machine learning models.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVoice command recognition:\u003c/strong\u003e Hands-free vehicle control through voice inputs.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWaypoint navigation:\u003c/strong\u003e GPS-based precise navigation to specific destinations.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time data collection:\u003c/strong\u003e Integration of telemetry and sensor data for system optimization.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-components\"\u003e\u003cstrong\u003eHardware Components:\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePrimary Hardware:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eRaspberry Pi 4\u003c/li\u003e\n\u003cli\u003eArduino\u003c/li\u003e\n\u003cli\u003eRaspberry Pi Camera with Night Vision Sensors\u003c/li\u003e\n\u003cli\u003eL298D Motor Drive\u003c/li\u003e\n\u003cli\u003eRaspberry Pi Microphone and Speaker\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAdditional Hardware:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUltrasonic Sensors\u003c/li\u003e\n\u003cli\u003eLiDAR/Radar\u003c/li\u003e\n\u003cli\u003eGPS Module\u003c/li\u003e\n\u003cli\u003eBluetooth Module\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"concepts-and-technologies\"\u003e\u003cstrong\u003eConcepts and Technologies:\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eComputer Vision:\u003c/strong\u003e Object detection, sign recognition, and real-time image processing using \u003cstrong\u003eOpenCV\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMachine Learning and Deep Learning:\u003c/strong\u003e TensorFlow and PyTorch-based models for sign classification and voice recognition.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGPS Navigation:\u003c/strong\u003e Waypoint-based navigation for precise route planning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eObstacle Avoidance:\u003c/strong\u003e Real-time distance measurement and navigation using ultrasonic sensors and LiDAR.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAuto Parking:\u003c/strong\u003e Automated parking algorithms for efficient maneuvering.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraffic Signboard Recognition:\u003c/strong\u003e AI-driven recognition of traffic signs to simulate real-world scenarios.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"programming-languages\"\u003e\u003cstrong\u003eProgramming Languages:\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePython:\u003c/strong\u003e (80%) Used for most AI, ML, and sensor data processing tasks.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eC++:\u003c/strong\u003e (20%) For low-level hardware integration and control.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eJava and CSS:\u003c/strong\u003e For GPS tracking through mobile and smartwatch applications\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"working-process\"\u003e\u003cstrong\u003eWorking Process:\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Input:\u003c/strong\u003e Real-time data collection from ultrasonic sensors, LiDAR, and cameras.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Processing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eObject detection and obstacle avoidance using OpenCV and machine learning models.\u003c/li\u003e\n\u003cli\u003eTraffic sign recognition and decision-making with TensorFlow 2.0.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVehicle Control:\u003c/strong\u003e Autonomous navigation and parking using processed sensor data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOutput and Monitoring:\u003c/strong\u003e Real-time visualization and monitoring via a local server (localhost).\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"applications\"\u003e\u003cstrong\u003eApplications:\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAutonomous driving research and development.\u003c/li\u003e\n\u003cli\u003eRobotics and intelligent automation.\u003c/li\u003e\n\u003cli\u003eSmart transportation systems.\u003c/li\u003e\n\u003cli\u003eAI-based real-time monitoring and data collection.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003cstrong\u003eAutonomous Driving Prototype\u003c/strong\u003e combines cutting-edge AI techniques with practical embedded systems to offer a versatile and scalable solution for autonomous navigation and control. This project demonstrates how emerging technologies can be used to bridge the gap between AI research and real-world applications.\u003c/p\u003e","title":"Autonomous Driving Prototype Using Embedded Systems and AI"},{"content":"Description This Real-Time Satellite Tracking and Trajectory Analysis project is a comprehensive system designed to monitor the International Space Station (ISS) in real time, visualize its trajectory, and analyze its past and future paths. The project offers interactive visualizations, anomaly detection capabilities, and insights into the ISS\u0026rsquo;s movement across Earth.\nKey Features Real-Time Tracking: Provides the exact location of the ISS in real time, letting users trace its current position over the Earth. Trajectory Visualization: Displays the ISS\u0026rsquo;s recent path and predicts future trajectories with stunning Earth visualizations. Day \u0026amp; Night Views: Incorporates real-time day-night boundary visualization, syncing with Earth\u0026rsquo;s rotation. Offers an understanding of how astronauts experience 16 sunrises and sunsets daily. Anomaly Detection: Analyzes the ISS\u0026rsquo;s trajectory to predict and highlight any anomalies in its path. Interactive Exploration: Allows users to explore the ISS\u0026rsquo;s majestic journey, fostering curiosity about space exploration. Working Process Data Acquisition: Retrieves real-time satellite data using Skyfield API. Data Processing: Processes orbital elements to calculate and predict the ISS\u0026rsquo;s position and trajectory. Visualization: Utilizes Cartopy for rendering Earth\u0026rsquo;s maps and the ISS\u0026rsquo;s path. Overlays the ISS\u0026rsquo;s position, trajectory, and day-night boundaries. Anomaly Analysis: Implements trajectory anomaly detection using historical data and predictions. Innovative Features Dynamic Earth Visualizations: Realistic views of Earth with synchronized day-night boundaries. Historical Trajectory Analysis: Displays the ISS\u0026rsquo;s past movements for a comprehensive context. Future Trajectory Predictions: Predicts the ISS\u0026rsquo;s upcoming path with precision. Tech Stack Programming Language: Python Libraries and Frameworks: Skyfield, Cartopy, Matplotlib Visualization Tools: Real-time map rendering, orbital path overlays Techniques: Trajectory Prediction, Anomaly Detection, Data Visualization Applications Space Exploration: Real-time insights into satellite movements for enthusiasts and researchers. Educational Tool: Simplifies the complexities of orbital mechanics for learners. Astronomy Integration: Supports sky-gazing and satellite spotting for astronomy lovers. Impact This project celebrates human curiosity and innovation by demystifying the ISS\u0026rsquo;s journey. By making satellite tracking accessible and interactive, it bridges the gap between technical data and public understanding. The next step involves integrating Artificial Intelligence for enhanced anomaly detection and predictive capabilities.\n","permalink":"http://localhost:1313/Akhil_Portfolio/projects/satellite/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThis \u003cstrong\u003eReal-Time Satellite Tracking and Trajectory Analysis\u003c/strong\u003e project is a comprehensive system designed to monitor the \u003cstrong\u003eInternational Space Station (ISS)\u003c/strong\u003e in real time, visualize its trajectory, and analyze its past and future paths. The project offers interactive visualizations, anomaly detection capabilities, and insights into the ISS\u0026rsquo;s movement across Earth.\u003c/p\u003e\n\u003ch3 id=\"key-features\"\u003e\u003cstrong\u003eKey Features\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eReal-Time Tracking:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eProvides the exact location of the ISS in real time, letting users trace its current position over the Earth.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTrajectory Visualization:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eDisplays the ISS\u0026rsquo;s recent path and predicts future trajectories with stunning Earth visualizations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDay \u0026amp; Night Views:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eIncorporates real-time \u003cstrong\u003eday-night boundary visualization\u003c/strong\u003e, syncing with Earth\u0026rsquo;s rotation.\u003c/li\u003e\n\u003cli\u003eOffers an understanding of how astronauts experience 16 sunrises and sunsets daily.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAnomaly Detection:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eAnalyzes the ISS\u0026rsquo;s trajectory to predict and highlight any anomalies in its path.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInteractive Exploration:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eAllows users to explore the ISS\u0026rsquo;s majestic journey, fostering curiosity about space exploration.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"working-process\"\u003e\u003cstrong\u003eWorking Process\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Acquisition:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eRetrieves real-time satellite data using \u003cstrong\u003eSkyfield API\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Processing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eProcesses orbital elements to calculate and predict the ISS\u0026rsquo;s position and trajectory.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVisualization:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUtilizes \u003cstrong\u003eCartopy\u003c/strong\u003e for rendering Earth\u0026rsquo;s maps and the ISS\u0026rsquo;s path.\u003c/li\u003e\n\u003cli\u003eOverlays the ISS\u0026rsquo;s position, trajectory, and day-night boundaries.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAnomaly Analysis:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eImplements trajectory anomaly detection using historical data and predictions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"innovative-features\"\u003e\u003cstrong\u003eInnovative Features\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDynamic Earth Visualizations:\u003c/strong\u003e Realistic views of Earth with synchronized day-night boundaries.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHistorical Trajectory Analysis:\u003c/strong\u003e Displays the ISS\u0026rsquo;s past movements for a comprehensive context.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFuture Trajectory Predictions:\u003c/strong\u003e Predicts the ISS\u0026rsquo;s upcoming path with precision.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"tech-stack\"\u003e\u003cstrong\u003eTech Stack\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProgramming Language:\u003c/strong\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLibraries and Frameworks:\u003c/strong\u003e Skyfield, Cartopy, Matplotlib\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVisualization Tools:\u003c/strong\u003e Real-time map rendering, orbital path overlays\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTechniques:\u003c/strong\u003e Trajectory Prediction, Anomaly Detection, Data Visualization\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"applications\"\u003e\u003cstrong\u003eApplications\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSpace Exploration:\u003c/strong\u003e Real-time insights into satellite movements for enthusiasts and researchers.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEducational Tool:\u003c/strong\u003e Simplifies the complexities of orbital mechanics for learners.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAstronomy Integration:\u003c/strong\u003e Supports sky-gazing and satellite spotting for astronomy lovers.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"impact\"\u003e\u003cstrong\u003eImpact\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eThis project celebrates human curiosity and innovation by demystifying the ISS\u0026rsquo;s journey. By making satellite tracking accessible and interactive, it bridges the gap between technical data and public understanding. The next step involves integrating \u003cstrong\u003eArtificial Intelligence\u003c/strong\u003e for enhanced anomaly detection and predictive capabilities.\u003c/p\u003e","title":"Real-Time Satellite Tracking and Trajectory Analysis"},{"content":"Get in Touch Feel free to reach out to me through the following channels:\n📞 Phone: (+49 15163661969) 📧 Email: Email 🔗 LinkedIn: linkedin Looking forward to connecting with you! 🚀\n","permalink":"http://localhost:1313/Akhil_Portfolio/contact/","summary":"\u003ch1 id=\"get-in-touch\"\u003eGet in Touch\u003c/h1\u003e\n\u003cp\u003eFeel free to reach out to me through the following channels:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e📞 \u003cstrong\u003ePhone\u003c/strong\u003e:  (+49 15163661969)\u003c/li\u003e\n\u003cli\u003e📧 \u003cstrong\u003eEmail\u003c/strong\u003e: \u003ca href=\"mailto:rakhilsai95@gmail.com\"\u003eEmail\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e🔗 \u003cstrong\u003eLinkedIn\u003c/strong\u003e: \u003ca href=\"https://www.linkedin.com/in/akhil-sai-ravi-kumar-287830b5/\"\u003elinkedin\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLooking forward to connecting with you! 🚀\u003c/p\u003e","title":"Contact"},{"content":"Description The Quadcopter Live Aerial Video Surveillance System is an advanced drone platform designed to provide a live aerial video feed for real-time monitoring and analysis. This system enables rapid deployment of aerial correspondence for critical operations like search and rescue, firefighting, law enforcement, military applications, and news reporting. With future-focused capabilities such as target tracking and video compression, the system is designed for scalability and integration into various industries.\nKey Features Live Aerial Video Feed: Captures and transmits real-time video from the drone to a remote controller or computer. Digital Video Processing: Converts captured video signals into digital format for analysis and storage. Remote and Voice Control: Operates the drone with remote control, with future expansions for voice control integration. Auto-Landing: Ensures safe and precise landing using flight control systems. Target Tracking and Video Compression (Future Expansion): Potential to integrate advanced algorithms for target identification and bandwidth-efficient video transmission. Components Quadcopter/Controller System: Frame: Durable structure to support all hardware components. DC Motors and Rotors: Provide propulsion and maneuverability. Power Supply: Battery-driven system for sustained operation. Flight Control Board: Manages stability and flight dynamics. Flight Control Software: Ensures smooth operation and maneuvering. RC Controller: Enables precise remote navigation. Video Transmission System: Camera: Captures high-resolution aerial footage. Transmitter and Receiver: Streams live video feed to the ground control station. Display: Visualizes the live feed in real-time on a monitor or computer. Digital Video Analysis System: Signal Processing: Converts raw camera signals into a digital format. Video Output: Displays the processed feed on connected devices. Applications Search and Rescue: Rapid deployment in disaster-hit areas to locate survivors and assess damage. Firefighting: Real-time aerial monitoring to track fire spread and aid firefighting strategies. Law Enforcement: Surveillance and monitoring for public safety and criminal investigations. Military: Tactical reconnaissance and monitoring in high-risk zones. News Reporting: Live aerial coverage of events, enabling faster reporting. Working Process Data Acquisition: Captures video feed via a mounted camera on the quadcopter. Data Transmission: Transfers the video signal to the receiver using a transmitter. Signal Conversion: Converts the received signal into a digital video format. Video Analysis: Analyzes and stores video data for further use, with future provisions for tracking and compression. Tech Stack Hardware: Quadcopter frame, DC motors, rotors, flight control system, and RC controller. Software: Digital signal processing, flight control software. Video Transmission: Camera and transmitter/receiver modules. Impact This project bridges the gap between traditional surveillance systems and modern drone technology. It provides a faster, cost-effective, and versatile alternative to conventional helicopters, making it an indispensable tool in emergency response and monitoring scenarios. The future integration of sentience, target tracking, and video compression opens avenues for more advanced applications.\n","permalink":"http://localhost:1313/Akhil_Portfolio/projects/drone/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eQuadcopter Live Aerial Video Surveillance System\u003c/strong\u003e is an advanced drone platform designed to provide a live aerial video feed for real-time monitoring and analysis. This system enables rapid deployment of aerial correspondence for critical operations like \u003cstrong\u003esearch and rescue, firefighting, law enforcement, military applications\u003c/strong\u003e, and \u003cstrong\u003enews reporting\u003c/strong\u003e. With future-focused capabilities such as \u003cstrong\u003etarget tracking\u003c/strong\u003e and \u003cstrong\u003evideo compression\u003c/strong\u003e, the system is designed for scalability and integration into various industries.\u003c/p\u003e","title":"Quadcopter Live Aerial Video Surveillance System"},{"content":"Description As a Software Developer and Research Engineer specializing inGenerative AI, AI, and Machine Learning at ASTRIAL GmbH, I have spearheaded the development of a co-pilot designed to analyze security alerts, reduce false positives, and prioritize alarms, streamlining workflows for less-experienced security personnel.\nMy responsibilities included extensive TTP data preprocessing, where I rephrased and shortened sentences to preserve their original meaning, enhancing model comprehension. Leveraging SecureBERT, I handled domain-specific terms to improve the detection and understanding of complex technical concepts in cybersecurity data. Additionally, I utilized GPT-3.5 to rephrase sentences, ensuring readability, diversity, and clarity while maintaining their core intent.\nTo enable intuitive threat analysis, I integrated preprocessed TTP data into Neo4j, visualizing TTPs and SubTTPs along with their relationships. Further, I incorporated Llama2 with the Neo4j GraphDatabase, empowering the co-pilot to guide users in navigating complex security datasets and identifying techniques and tactics effectively.\nMy expertise spans deep learning models such as BERT, GPT, T5, and OpenAI APIs, applied to tasks like text classification, summarization, and question-answering. I have fine-tuned LLMs (e.g., Llama2, GPT) for domain-specific applications, applied prompt engineering to optimize LLM outputs, and implemented cosine similarity and vector embeddings for tasks like Named Entity Recognition (NER) and semantic search.\nIn addition to my NLP expertise, I have delivered end-to-end image processing solutions, including data acquisition, preprocessing, model training, and deployment. I enhanced object detection and segmentation algorithms using tools like YoloV5, OpenCV, and SAM (Segment Anything Model) while optimizing models for edge device deployments with ONNX and OpenVINO.\nFurther accomplishments include designing sound detection algorithms using FFT and geo-localization techniques, applying advanced audio preprocessing for signal quality optimization, and deploying applications with Docker-based CI/CD pipelines for scalable, reliable delivery. My focus on model robustness and data optimization techniques has ensured consistent, high-quality outputs.\nI also contributed to team development by mentoring interns and master’s students, fostering technical excellence and collaborative learning. Through a combination of technical innovation and mentorship, I have driven impactful solutions in AI-powered cybersecurity, computer vision, and generative AI.\n","permalink":"http://localhost:1313/Akhil_Portfolio/experience/astrial/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs a \u003cstrong\u003eSoftware Developer and Research Engineer\u003c/strong\u003e specializing in\u003cstrong\u003eGenerative AI, AI, and Machine Learning\u003c/strong\u003e at \u003cstrong\u003eASTRIAL GmbH\u003c/strong\u003e, I have spearheaded the development of a \u003cstrong\u003eco-pilot\u003c/strong\u003e designed to analyze security alerts, reduce false positives, and prioritize alarms, streamlining workflows for less-experienced security personnel.\u003c/p\u003e\n\u003cp\u003eMy responsibilities included extensive TTP data preprocessing, where I rephrased and shortened sentences to preserve their original meaning, enhancing model comprehension. Leveraging \u003cstrong\u003eSecureBERT\u003c/strong\u003e, I handled domain-specific terms to improve the detection and understanding of complex technical concepts in \u003cstrong\u003ecybersecurity\u003c/strong\u003e data. Additionally, I utilized \u003cstrong\u003eGPT-3.5\u003c/strong\u003e to rephrase sentences, ensuring readability, diversity, and clarity while maintaining their core intent.\u003c/p\u003e","title":"Senior Software Developer and Research engineer"},{"content":"Description As a Python Developer specializing in Simulation Frameworks, UI Development, and Machine Learning, I contributed to building advanced systems for demand-driven environments and predictive modeling. My work focused on enhancing supply chain management tools, developing user-friendly interfaces, and creating machine learning models to improve forecasting and decision-making processes.\nI designed and implemented an improved User Interface (UI) for DDMRP (Demand-Driven Material Requirements Planning) and MRP (Material Requirements Planning) engines, optimizing usability and operational efficiency in supply chain systems. Leveraging Flask, I built a dynamic web application to display, edit, and manage input values for simulation engines, including WMAPE, DDMRP, and MRP, enabling real-time updates and streamlined data handling.\nTo standardize and accelerate development, I developed custom reusable User Controls in Flask, ensuring consistent and efficient design across multiple screens. I also architected key components such as the Data Access Layer (DAL) and Business Logic Layer (BLL), ensuring robust backend architecture and seamless data flow.\nIn the domain of security and authentication, I implemented secure user sign-in and sign-up mechanisms, integrating third-party authentication providers like Google and Microsoft to enhance application security. Additionally, I developed a WMAPE automatic error identification system, incorporating heatmap graphs to provide intuitive visual diagnostics for demand generator engines.\nI also contributed to machine learning model development, building models for time series data generation and future demand prediction. By utilizing algorithms such as Random Forest, Isolation Forest, and LSTM neural networks, I improved forecasting accuracy and reliability in dynamic demand environments.\nThrough performance optimization and troubleshooting, I ensured seamless operation and an enhanced user experience across all modules. My technical expertise in UI development, backend architecture, and machine learning enabled me to deliver impactful solutions for simulation frameworks and demand-driven applications.\n","permalink":"http://localhost:1313/Akhil_Portfolio/experience/camelot/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs a \u003cstrong\u003ePython Developer\u003c/strong\u003e specializing in Simulation \u003cstrong\u003eFrameworks, UI Development\u003c/strong\u003e, and \u003cstrong\u003eMachine Learning\u003c/strong\u003e, I contributed to building advanced systems for demand-driven environments and predictive modeling. My work focused on enhancing supply chain management tools, developing user-friendly interfaces, and creating machine learning models to improve forecasting and decision-making processes.\u003c/p\u003e\n\u003cp\u003eI designed and implemented an improved User Interface (\u003cstrong\u003eUI\u003c/strong\u003e) for \u003cstrong\u003eDDMRP\u003c/strong\u003e (Demand-Driven Material Requirements Planning) and \u003cstrong\u003eMRP\u003c/strong\u003e (Material Requirements Planning) engines, optimizing usability and operational efficiency in supply chain systems. Leveraging \u003cstrong\u003eFlask\u003c/strong\u003e, I built a dynamic web application to display, edit, and manage input values for simulation engines, including \u003cstrong\u003eWMAPE, DDMRP, and MRP, enabling real-time updates\u003c/strong\u003e and \u003cstrong\u003estreamlined data handling\u003c/strong\u003e.\u003c/p\u003e","title":"Python Developer (Intern)"},{"content":"Description As a Software Design and Developer Engineer at Tech Mahindra LTD, I worked extensively on designing, developing, and deploying software solutions leveraging technologies like Python, AWS, .NET, HTML, CSS, JavaScript, and Microsoft Azure Cloud. My role involved automation, responsive web design, cloud infrastructure management, and enhancing application performance for seamless operations.\nI automated Azure resource creation using PowerShell scripts, managing resource groups, web applications, storage blobs/tables, and firewall rules to optimize cloud deployment processes. Additionally, I developed responsive web designs (RWD) using Media Queries, Bootstrap, HTML, CSS, and JavaScript, ensuring cross-device compatibility and delivering consistent user experiences across platforms.\nIn application development and deployment, I configured and managed IIS Web Servers for hosting and collaborated in system specification meetings to align development with user requirements. I designed and implemented CI/CD pipelines using AWS CodePipeline, CodeCommit, and CodeDeploy, streamlining continuous integration and delivery for efficient software deployment.\nMy expertise extended to cloud infrastructure management, where I deployed and maintained Azure IaaS virtual machines (VMs), PaaS role instances, and AWS Elastic Beanstalk environments, ensuring scalable and reliable infrastructure. I also managed version control systems using GIT, GitHub, and TFS, employing structured branching strategies to maintain codebase integrity and foster collaboration.\nAs part of technical troubleshooting, I diagnosed and resolved application performance issues, verified data integrity, and identified code defects. I implemented SSO policies and federated identities, managing dynamic group memberships and user access policies across multiple SaaS applications to enhance security and identity management.\nFurthermore, I collaborated on database functionalities, data analytics, and reporting requirements, ensuring alignment with organizational objectives. My comprehensive skill set in software development, cloud automation, and infrastructure management enabled me to deliver high-impact solutions for enterprise-grade applications.\n","permalink":"http://localhost:1313/Akhil_Portfolio/experience/tech-mahindra-ltd/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs a Software Design and Developer Engineer at \u003cstrong\u003eTech Mahindra LTD\u003c/strong\u003e, I worked extensively on designing, developing, and deploying software solutions leveraging technologies like Python, AWS, .NET, HTML, CSS, JavaScript, and Microsoft Azure Cloud. My role involved \u003cstrong\u003eautomation, responsive web design, cloud infrastructure management\u003c/strong\u003e, and \u003cstrong\u003eenhancing application performance\u003c/strong\u003e for seamless operations.\u003c/p\u003e\n\u003cp\u003eI automated Azure resource creation using PowerShell scripts, managing resource groups, web applications, storage blobs/tables, and firewall rules to optimize cloud deployment processes. Additionally, I developed responsive web designs (\u003cstrong\u003eRWD\u003c/strong\u003e) using \u003cstrong\u003eMedia Queries, Bootstrap, HTML, CSS\u003c/strong\u003e, and \u003cstrong\u003eJavaScript\u003c/strong\u003e, ensuring cross-device compatibility and delivering consistent user experiences across platforms.\u003c/p\u003e","title":"Software Design and Developer Engineer"},{"content":"Description As a Junior Software Developer (Intern) specializing in AI, ML, and Robotics, I played a pivotal role in designing and developing a prototype smart rover named as MOWME, leveraging AI, ML, and DL technologies to enable autonomous operations. The rover was equipped with SLAM (Simultaneous Localization and Mapping) capabilities and utilized CNN networks for task identification and classification. I integrated advanced features such as object detection, obstacle avoidance, GPS waypoint generation, and self-testing mechanisms, achieving seamless autonomous navigation.\nTo ensure high-quality datasets, I built automated data preprocessing pipelines, incorporating steps like Normalization, Standardization, Resizing, Denoising, Segmentation, and Morphology, as well as JSON creation and Data Pickling for efficient data handling. These pipelines enabled consistent and reliable input data for machine learning workflows.\nIn the domain of Machine Learning and Model Optimization, I implemented Regression, Classification, and Ensemble algorithms, troubleshooting bottlenecks to enhance model accuracy and reduce inference latency. Optimized models were deployed on edge devices such as Raspberry Pi 3, Jetson Nano, and AWS EC2 GPU instances, leveraging GPU acceleration with tools like CUDA and cuDNN for efficient performance.\nAdditionally, I integrated IoT devices like TCG4 sensors and Beacon devices to collect metrics (e.g., acceleration, pressure, temperature, humidity) and established cloud communication pipelines with AWS. I worked with advanced communication protocols including SOMEIP, VSOMEIP, and CAN to ensure robust and stable device connectivity.\nMy contributions also included designing and developing a web-based AI model development platform, enabling users to build image-based AI models for object detection, classification, and recognition without requiring programming expertise.\nThrough effective CI/CD pipeline management and version control systems like GitHub and GitLab, I ensured streamlined workflows, reliable deployments, and system scalability. My work combined technical expertise and innovation to deliver a versatile smart rover and empower users with accessible AI-driven solutions.\n","permalink":"http://localhost:1313/Akhil_Portfolio/experience/avl-software-and-functions-gmbh/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eAs a \u003cstrong\u003eJunior Software Developer\u003c/strong\u003e (Intern) specializing in \u003cstrong\u003eAI, ML\u003c/strong\u003e, and \u003cstrong\u003eRobotics\u003c/strong\u003e, I played a pivotal role in designing and developing a prototype \u003cstrong\u003esmart rover\u003c/strong\u003e named as \u003cstrong\u003eMOWME\u003c/strong\u003e, leveraging AI, ML, and DL technologies to enable autonomous operations. The rover was equipped with \u003cstrong\u003eSLAM\u003c/strong\u003e (Simultaneous Localization and Mapping) capabilities and utilized CNN networks for task identification and classification. I integrated advanced features such as \u003cstrong\u003eobject detection\u003c/strong\u003e, \u003cstrong\u003eobstacle avoidance\u003c/strong\u003e, \u003cstrong\u003eGPS\u003c/strong\u003e waypoint generation, and self-testing mechanisms, achieving seamless autonomous navigation.\u003c/p\u003e","title":"Junior Software Developer (Intern)"},{"content":"Description This Web-Based CNN Model Tester and Trainer is an intuitive application designed to simplify AI model development for image-based tasks like object detection, classification, and recognition. The platform enables users to create, train, and test AI models without requiring expertise in programming or machine learning.\nKey Features Automated Data Preprocessing: Handles cleaning, augmentation, and preparation of image datasets. End-to-End Model Training: Trains Convolutional Neural Network (CNN) models using uploaded datasets. Model Testing: Allows users to upload images or capture them in real time for testing model predictions. Model Deployment: Provides the trained model in formats like H5, JSON, ready for deployment on edge devices. Real-Time Dataset Capture: Supports capturing datasets directly via webcam or connected cameras. User-Friendly Workflow: Offers a no-code experience for AI enthusiasts, researchers, and developers. Working Process Data Input: Users can upload datasets or capture real-time images for training and testing. Data Preprocessing: The tool cleans and prepares the data automatically, ensuring high-quality inputs. Model Training: CNN models are trained using TensorFlow, OpenVINO, or ONNX. Model Testing: Users can test the model on uploaded images or real-time captures. Model Export: Trained models are made available in formats such as H5, JSON, and ONNX for deployment. Applications Autonomous Vehicle Development: Used by AVL Software and Functions GmbH for ADAS (Advanced Driver Assistance Systems) and autonomous vehicle projects. AI Learning and Research: Makes AI accessible for researchers and enthusiasts by eliminating the need for advanced programming skills. Custom AI Solutions: Suitable for creating tailor-made AI models for object detection and recognition. Tech Stack Programming Languages: Python, HTML, CSS, JavaScript. Frameworks and Libraries: TensorFlow, Keras, OpenCV, TensorflowJS, Flask, FAST API. Tools and Technologies: Protobuf, OpenVINO, ONNX, TensorBoard, CUDA, cuDNN. Neural Network: Convolutional Neural Network (CNN). Impact The primary aim of this project is to make AI model development easy, quick, and accessible, empowering users with limited technical knowledge to develop AI solutions independently. The platform’s deployment at AVL Software and Functions GmbH demonstrates its effectiveness in supporting ADAS and autonomous vehicle projects.\n","permalink":"http://localhost:1313/Akhil_Portfolio/projects/master_thesis/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThis \u003cstrong\u003eWeb-Based CNN Model Tester and Trainer\u003c/strong\u003e is an intuitive application designed to simplify AI model development for image-based tasks like \u003cstrong\u003eobject detection, classification, and recognition\u003c/strong\u003e. The platform enables users to create, train, and test AI models without requiring expertise in programming or machine learning.\u003c/p\u003e\n\u003ch3 id=\"key-features\"\u003e\u003cstrong\u003eKey Features\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eAutomated Data Preprocessing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eHandles cleaning, augmentation, and preparation of image datasets.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEnd-to-End Model Training:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eTrains Convolutional Neural Network (CNN) models using uploaded datasets.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel Testing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eAllows users to upload images or capture them in real time for testing model predictions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel Deployment:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eProvides the trained model in formats like \u003cstrong\u003eH5, JSON\u003c/strong\u003e, ready for deployment on edge devices.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReal-Time Dataset Capture:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eSupports capturing datasets directly via webcam or connected cameras.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUser-Friendly Workflow:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eOffers a no-code experience for AI enthusiasts, researchers, and developers.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"working-process\"\u003e\u003cstrong\u003eWorking Process\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Input:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUsers can upload datasets or capture real-time images for training and testing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Preprocessing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eThe tool cleans and prepares the data automatically, ensuring high-quality inputs.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel Training:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eCNN models are trained using TensorFlow, OpenVINO, or ONNX.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel Testing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUsers can test the model on uploaded images or real-time captures.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel Export:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eTrained models are made available in formats such as H5, JSON, and ONNX for deployment.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"applications\"\u003e\u003cstrong\u003eApplications\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAutonomous Vehicle Development:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUsed by AVL Software and Functions GmbH for ADAS (Advanced Driver Assistance Systems) and autonomous vehicle projects.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAI Learning and Research:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eMakes AI accessible for researchers and enthusiasts by eliminating the need for advanced programming skills.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCustom AI Solutions:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eSuitable for creating tailor-made AI models for object detection and recognition.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"tech-stack\"\u003e\u003cstrong\u003eTech Stack\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProgramming Languages:\u003c/strong\u003e Python, HTML, CSS, JavaScript.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFrameworks and Libraries:\u003c/strong\u003e TensorFlow, Keras, OpenCV, TensorflowJS, Flask, FAST API.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTools and Technologies:\u003c/strong\u003e Protobuf, OpenVINO, ONNX, TensorBoard, CUDA, cuDNN.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNeural Network:\u003c/strong\u003e Convolutional Neural Network (CNN).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"impact\"\u003e\u003cstrong\u003eImpact\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eThe primary aim of this project is to make AI model development \u003cstrong\u003eeasy, quick, and accessible\u003c/strong\u003e, empowering users with limited technical knowledge to develop AI solutions independently. The platform’s deployment at AVL Software and Functions GmbH demonstrates its effectiveness in supporting \u003cstrong\u003eADAS and autonomous vehicle projects\u003c/strong\u003e.\u003c/p\u003e","title":"Web-Based CNN Model Tester and Trainer"},{"content":"Description Project MOWME is an innovative Autonomous Driving Rover prototype designed to showcase advanced AI, ML, and IoT-driven capabilities. The rover is built to autonomously navigate, classify objects, collect and self-train data, and transfer insights to the cloud for future learning. This system integrates advanced hardware with cloud storage and machine learning for real-time decision-making and environmental analysis.\nKey Features Object Classification: The rover uses machine learning to identify and classify objects in its environment. Auto-Data Collection: Captures real-time sensor and camera data, processing it locally or storing it in the cloud. Self-Training: Implements feedback loops to refine object classification and navigation algorithms over time. Auto-Navigation: Incorporates GPS waypoint navigation for autonomous movement across dynamic terrains. Obstacle Avoidance: Uses ultrasonic sensors for collision detection and avoidance. Cloud Integration: Data is stored on AWS Cloud, enabling future training and analysis. Environmental Monitoring: Detects diseases in grass using Convolutional Neural Networks (CNN), identifying patterns like dryness or overwatering. Hardware Components Core Components: Raspberry Pi 4 Arduino Raspberry Pi Camera L298D Motor Drive Additional Sensors: Ultrasonic Sensors GPS Module Bluetooth Module Software Features Wireless Drive Control: The rover can be controlled using MQTT protocol for wireless operation. Video Streaming: Real-time video streaming from Raspberry Pi to a laptop for monitoring. Dataset Collection: Automatically captures and processes images for model training, storing them locally or in the cloud. Disease Detection in Grass: Leverages CNN models to analyze grass conditions and classify diseases. Working Process Data Collection: Captures sensor data, camera images, and ultrasonic measurements. Transfers data to AWS Cloud for processing and storage. Data Processing: Object classification and dataset analysis using TensorFlow and CNN models. Disease detection in grass based on visual patterns. Navigation and Control: Utilizes GPS and sensor data for waypoint navigation. Avoids obstacles dynamically with ultrasonic sensors. Wireless Communication: MQTT protocol enables seamless communication between devices. Live monitoring of rover operations via Raspberry Pi. Programming Languages Python: (80%) Core logic, machine learning models, and data processing. C++: (20%) For low-level hardware interaction and control. AWS Integration: IoT and cloud-based storage for scalability. Applications Autonomous lawn management systems. Environmental monitoring and data collection. IoT-driven robotics for agriculture. AI-based disease detection and analytics. The MOWME Rover demonstrates the seamless integration of hardware, AI, and cloud services, offering a versatile solution for autonomous systems and environmental analysis. This project is a step forward in creating smart, scalable, and adaptive IoT-enabled devices for practical use cases.\n","permalink":"http://localhost:1313/Akhil_Portfolio/projects/mowme/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eProject \u003cstrong\u003eMOWME\u003c/strong\u003e is an innovative \u003cstrong\u003eAutonomous Driving Rover\u003c/strong\u003e prototype designed to showcase advanced AI, ML, and IoT-driven capabilities. The rover is built to autonomously navigate, classify objects, collect and self-train data, and transfer insights to the cloud for future learning. This system integrates advanced hardware with cloud storage and machine learning for real-time decision-making and environmental analysis.\u003c/p\u003e\n\u003ch3 id=\"key-features\"\u003e\u003cstrong\u003eKey Features\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eObject Classification:\u003c/strong\u003e The rover uses machine learning to identify and classify objects in its environment.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAuto-Data Collection:\u003c/strong\u003e Captures real-time sensor and camera data, processing it locally or storing it in the cloud.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Training:\u003c/strong\u003e Implements feedback loops to refine object classification and navigation algorithms over time.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAuto-Navigation:\u003c/strong\u003e Incorporates \u003cstrong\u003eGPS waypoint navigation\u003c/strong\u003e for autonomous movement across dynamic terrains.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eObstacle Avoidance:\u003c/strong\u003e Uses ultrasonic sensors for collision detection and avoidance.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCloud Integration:\u003c/strong\u003e Data is stored on \u003cstrong\u003eAWS Cloud\u003c/strong\u003e, enabling future training and analysis.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEnvironmental Monitoring:\u003c/strong\u003e Detects diseases in grass using \u003cstrong\u003eConvolutional Neural Networks (CNN)\u003c/strong\u003e, identifying patterns like dryness or overwatering.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"hardware-components\"\u003e\u003cstrong\u003eHardware Components\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCore Components:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eRaspberry Pi 4\u003c/li\u003e\n\u003cli\u003eArduino\u003c/li\u003e\n\u003cli\u003eRaspberry Pi Camera\u003c/li\u003e\n\u003cli\u003eL298D Motor Drive\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAdditional Sensors:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUltrasonic Sensors\u003c/li\u003e\n\u003cli\u003eGPS Module\u003c/li\u003e\n\u003cli\u003eBluetooth Module\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"software-features\"\u003e\u003cstrong\u003eSoftware Features\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eWireless Drive Control:\u003c/strong\u003e The rover can be controlled using MQTT protocol for wireless operation.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVideo Streaming:\u003c/strong\u003e Real-time video streaming from Raspberry Pi to a laptop for monitoring.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDataset Collection:\u003c/strong\u003e Automatically captures and processes images for model training, storing them locally or in the cloud.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDisease Detection in Grass:\u003c/strong\u003e Leverages CNN models to analyze grass conditions and classify diseases.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"working-process\"\u003e\u003cstrong\u003eWorking Process\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Collection:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eCaptures sensor data, camera images, and ultrasonic measurements.\u003c/li\u003e\n\u003cli\u003eTransfers data to AWS Cloud for processing and storage.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Processing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eObject classification and dataset analysis using \u003cstrong\u003eTensorFlow\u003c/strong\u003e and \u003cstrong\u003eCNN models\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eDisease detection in grass based on visual patterns.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNavigation and Control:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUtilizes GPS and sensor data for waypoint navigation.\u003c/li\u003e\n\u003cli\u003eAvoids obstacles dynamically with ultrasonic sensors.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWireless Communication:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eMQTT protocol enables seamless communication between devices.\u003c/li\u003e\n\u003cli\u003eLive monitoring of rover operations via Raspberry Pi.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"programming-languages\"\u003e\u003cstrong\u003eProgramming Languages\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePython:\u003c/strong\u003e (80%) Core logic, machine learning models, and data processing.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eC++:\u003c/strong\u003e (20%) For low-level hardware interaction and control.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAWS Integration:\u003c/strong\u003e IoT and cloud-based storage for scalability.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"applications\"\u003e\u003cstrong\u003eApplications\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAutonomous lawn management systems.\u003c/li\u003e\n\u003cli\u003eEnvironmental monitoring and data collection.\u003c/li\u003e\n\u003cli\u003eIoT-driven robotics for agriculture.\u003c/li\u003e\n\u003cli\u003eAI-based disease detection and analytics.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003cstrong\u003eMOWME Rover\u003c/strong\u003e demonstrates the seamless integration of hardware, AI, and cloud services, offering a versatile solution for autonomous systems and environmental analysis. This project is a step forward in creating smart, scalable, and adaptive IoT-enabled devices for practical use cases.\u003c/p\u003e","title":"Autonomous Driving Rover: Project MOWME"},{"content":"Description This project focuses on developing an intelligent Assets and Alerts Copilot system that leverages graph-based data modeling and AI-powered insights to provide a comprehensive cybersecurity solution. The system integrates advanced tools like Neo4j, Mistral, and Streamlit to enable real-time asset and alert detection, relationship mapping, and contextual analysis.\nThe graph-based data modeling ensures accurate visualization of assets, products, vulnerabilities, and their relationships, providing unparalleled clarity in understanding interconnected cybersecurity risks. With the Mistral LLM, the copilot allows users to interact using natural language queries, transforming complex data into actionable insights.\nThe system provides real-time responses to queries, dynamic alert prioritization, and customizable visualizations of vulnerabilities and asset connections. It is particularly valuable for security teams seeking to reduce alert fatigue, understand critical relationships, and prioritize actionable insights.\nThis project represents a cutting-edge approach to cybersecurity operations, combining advanced AI with graph-driven analysis to enhance threat detection, incident response, and asset management.\nTechnologies Used Programming Languages: Python Libraries and Frameworks: GCP Functions, Neo4J, Streamlit Applications: IT security team ","permalink":"http://localhost:1313/Akhil_Portfolio/projects/assets-and-alerts/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThis project focuses on developing an \u003cstrong\u003eintelligent Assets and Alerts Copilot system\u003c/strong\u003e that leverages graph-based data modeling and AI-powered insights to provide a comprehensive cybersecurity solution. The system integrates advanced tools like \u003cstrong\u003eNeo4j\u003c/strong\u003e, \u003cstrong\u003eMistral\u003c/strong\u003e, and \u003cstrong\u003eStreamlit\u003c/strong\u003e to enable real-time asset and alert detection, relationship mapping, and contextual analysis.\u003c/p\u003e\n\u003cp\u003eThe \u003cstrong\u003egraph-based data modeling\u003c/strong\u003e ensures accurate visualization of assets, products, vulnerabilities, and their relationships, providing unparalleled clarity in understanding interconnected cybersecurity risks. With the \u003cstrong\u003eMistral LLM\u003c/strong\u003e, the copilot allows users to interact using natural language queries, transforming complex data into actionable insights.\u003c/p\u003e","title":"Assets and Alerts Copilot Using Graph-Based AI Systems"},{"content":"Description This project showcases the development of an Autonomous Vehicle System (AVS) prototype, integrating advanced AI, Machine Learning (ML), and Deep Learning (DL) techniques with embedded systems. The project leverages a combination of Raspberry Pi 4, Arduino, and NVIDIA Jetson Nano/TX2 NX, along with additional hardware components, to create a scalable and efficient system that simulates real-world autonomous vehicle capabilities.\nKey Features: Self-driving: Autonomous navigation with path following. Obstacle avoidance: Detection and navigation around obstacles using ultrasonic sensors and LiDAR/Radar. Self-parking: Automated parking into predefined spaces. Traffic signboard recognition: Classification of traffic signs using machine learning models. Voice command recognition: Hands-free vehicle control through voice inputs. Waypoint navigation: GPS-based precise navigation to specific destinations. Real-time data collection: Integration of telemetry and sensor data for system optimization. Hardware Components: Primary Hardware: Raspberry Pi 4 Arduino Raspberry Pi Camera with Night Vision Sensors L298D Motor Drive Raspberry Pi Microphone and Speaker Additional Hardware: Ultrasonic Sensors LiDAR/Radar GPS Module Bluetooth Module Concepts and Technologies: Computer Vision: Object detection, sign recognition, and real-time image processing using OpenCV. Machine Learning and Deep Learning: TensorFlow and PyTorch-based models for sign classification and voice recognition. GPS Navigation: Waypoint-based navigation for precise route planning. Obstacle Avoidance: Real-time distance measurement and navigation using ultrasonic sensors and LiDAR. Auto Parking: Automated parking algorithms for efficient maneuvering. Traffic Signboard Recognition: AI-driven recognition of traffic signs to simulate real-world scenarios. Programming Languages: Python: (80%) Used for most AI, ML, and sensor data processing tasks. C++: (20%) For low-level hardware integration and control. Java and CSS: For GPS tracking through mobile and smartwatch applications Working Process: Data Input: Real-time data collection from ultrasonic sensors, LiDAR, and cameras. Data Processing: Object detection and obstacle avoidance using OpenCV and machine learning models. Traffic sign recognition and decision-making with TensorFlow 2.0. Vehicle Control: Autonomous navigation and parking using processed sensor data. Output and Monitoring: Real-time visualization and monitoring via a local server (localhost). Applications: Autonomous driving research and development. Robotics and intelligent automation. Smart transportation systems. AI-based real-time monitoring and data collection. The Autonomous Driving Prototype combines cutting-edge AI techniques with practical embedded systems to offer a versatile and scalable solution for autonomous navigation and control. This project demonstrates how emerging technologies can be used to bridge the gap between AI research and real-world applications.\n","permalink":"http://localhost:1313/Akhil_Portfolio/projects/autonomus_vehicle/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThis project showcases the development of an \u003cstrong\u003eAutonomous Vehicle System (AVS) prototype\u003c/strong\u003e, integrating advanced \u003cstrong\u003eAI, Machine Learning (ML), and Deep Learning (DL)\u003c/strong\u003e techniques with embedded systems. The project leverages a combination of \u003cstrong\u003eRaspberry Pi 4\u003c/strong\u003e, \u003cstrong\u003eArduino\u003c/strong\u003e, and \u003cstrong\u003eNVIDIA Jetson Nano/TX2 NX\u003c/strong\u003e, along with additional hardware components, to create a scalable and efficient system that simulates real-world autonomous vehicle capabilities.\u003c/p\u003e\n\u003ch3 id=\"key-features\"\u003e\u003cstrong\u003eKey Features:\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-driving:\u003c/strong\u003e Autonomous navigation with path following.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eObstacle avoidance:\u003c/strong\u003e Detection and navigation around obstacles using ultrasonic sensors and LiDAR/Radar.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-parking:\u003c/strong\u003e Automated parking into predefined spaces.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraffic signboard recognition:\u003c/strong\u003e Classification of traffic signs using machine learning models.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVoice command recognition:\u003c/strong\u003e Hands-free vehicle control through voice inputs.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWaypoint navigation:\u003c/strong\u003e GPS-based precise navigation to specific destinations.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time data collection:\u003c/strong\u003e Integration of telemetry and sensor data for system optimization.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-components\"\u003e\u003cstrong\u003eHardware Components:\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePrimary Hardware:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eRaspberry Pi 4\u003c/li\u003e\n\u003cli\u003eArduino\u003c/li\u003e\n\u003cli\u003eRaspberry Pi Camera with Night Vision Sensors\u003c/li\u003e\n\u003cli\u003eL298D Motor Drive\u003c/li\u003e\n\u003cli\u003eRaspberry Pi Microphone and Speaker\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAdditional Hardware:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUltrasonic Sensors\u003c/li\u003e\n\u003cli\u003eLiDAR/Radar\u003c/li\u003e\n\u003cli\u003eGPS Module\u003c/li\u003e\n\u003cli\u003eBluetooth Module\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"concepts-and-technologies\"\u003e\u003cstrong\u003eConcepts and Technologies:\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eComputer Vision:\u003c/strong\u003e Object detection, sign recognition, and real-time image processing using \u003cstrong\u003eOpenCV\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMachine Learning and Deep Learning:\u003c/strong\u003e TensorFlow and PyTorch-based models for sign classification and voice recognition.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGPS Navigation:\u003c/strong\u003e Waypoint-based navigation for precise route planning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eObstacle Avoidance:\u003c/strong\u003e Real-time distance measurement and navigation using ultrasonic sensors and LiDAR.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAuto Parking:\u003c/strong\u003e Automated parking algorithms for efficient maneuvering.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraffic Signboard Recognition:\u003c/strong\u003e AI-driven recognition of traffic signs to simulate real-world scenarios.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"programming-languages\"\u003e\u003cstrong\u003eProgramming Languages:\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePython:\u003c/strong\u003e (80%) Used for most AI, ML, and sensor data processing tasks.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eC++:\u003c/strong\u003e (20%) For low-level hardware integration and control.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eJava and CSS:\u003c/strong\u003e For GPS tracking through mobile and smartwatch applications\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"working-process\"\u003e\u003cstrong\u003eWorking Process:\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Input:\u003c/strong\u003e Real-time data collection from ultrasonic sensors, LiDAR, and cameras.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Processing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eObject detection and obstacle avoidance using OpenCV and machine learning models.\u003c/li\u003e\n\u003cli\u003eTraffic sign recognition and decision-making with TensorFlow 2.0.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVehicle Control:\u003c/strong\u003e Autonomous navigation and parking using processed sensor data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOutput and Monitoring:\u003c/strong\u003e Real-time visualization and monitoring via a local server (localhost).\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"applications\"\u003e\u003cstrong\u003eApplications:\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAutonomous driving research and development.\u003c/li\u003e\n\u003cli\u003eRobotics and intelligent automation.\u003c/li\u003e\n\u003cli\u003eSmart transportation systems.\u003c/li\u003e\n\u003cli\u003eAI-based real-time monitoring and data collection.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003cstrong\u003eAutonomous Driving Prototype\u003c/strong\u003e combines cutting-edge AI techniques with practical embedded systems to offer a versatile and scalable solution for autonomous navigation and control. This project demonstrates how emerging technologies can be used to bridge the gap between AI research and real-world applications.\u003c/p\u003e","title":"Autonomous Driving Prototype Using Embedded Systems and AI"},{"content":"Description This Real-Time Satellite Tracking and Trajectory Analysis project is a comprehensive system designed to monitor the International Space Station (ISS) in real time, visualize its trajectory, and analyze its past and future paths. The project offers interactive visualizations, anomaly detection capabilities, and insights into the ISS\u0026rsquo;s movement across Earth.\nKey Features Real-Time Tracking: Provides the exact location of the ISS in real time, letting users trace its current position over the Earth. Trajectory Visualization: Displays the ISS\u0026rsquo;s recent path and predicts future trajectories with stunning Earth visualizations. Day \u0026amp; Night Views: Incorporates real-time day-night boundary visualization, syncing with Earth\u0026rsquo;s rotation. Offers an understanding of how astronauts experience 16 sunrises and sunsets daily. Anomaly Detection: Analyzes the ISS\u0026rsquo;s trajectory to predict and highlight any anomalies in its path. Interactive Exploration: Allows users to explore the ISS\u0026rsquo;s majestic journey, fostering curiosity about space exploration. Working Process Data Acquisition: Retrieves real-time satellite data using Skyfield API. Data Processing: Processes orbital elements to calculate and predict the ISS\u0026rsquo;s position and trajectory. Visualization: Utilizes Cartopy for rendering Earth\u0026rsquo;s maps and the ISS\u0026rsquo;s path. Overlays the ISS\u0026rsquo;s position, trajectory, and day-night boundaries. Anomaly Analysis: Implements trajectory anomaly detection using historical data and predictions. Innovative Features Dynamic Earth Visualizations: Realistic views of Earth with synchronized day-night boundaries. Historical Trajectory Analysis: Displays the ISS\u0026rsquo;s past movements for a comprehensive context. Future Trajectory Predictions: Predicts the ISS\u0026rsquo;s upcoming path with precision. Tech Stack Programming Language: Python Libraries and Frameworks: Skyfield, Cartopy, Matplotlib Visualization Tools: Real-time map rendering, orbital path overlays Techniques: Trajectory Prediction, Anomaly Detection, Data Visualization Applications Space Exploration: Real-time insights into satellite movements for enthusiasts and researchers. Educational Tool: Simplifies the complexities of orbital mechanics for learners. Astronomy Integration: Supports sky-gazing and satellite spotting for astronomy lovers. Impact This project celebrates human curiosity and innovation by demystifying the ISS\u0026rsquo;s journey. By making satellite tracking accessible and interactive, it bridges the gap between technical data and public understanding. The next step involves integrating Artificial Intelligence for enhanced anomaly detection and predictive capabilities.\n","permalink":"http://localhost:1313/Akhil_Portfolio/projects/satellite/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThis \u003cstrong\u003eReal-Time Satellite Tracking and Trajectory Analysis\u003c/strong\u003e project is a comprehensive system designed to monitor the \u003cstrong\u003eInternational Space Station (ISS)\u003c/strong\u003e in real time, visualize its trajectory, and analyze its past and future paths. The project offers interactive visualizations, anomaly detection capabilities, and insights into the ISS\u0026rsquo;s movement across Earth.\u003c/p\u003e\n\u003ch3 id=\"key-features\"\u003e\u003cstrong\u003eKey Features\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eReal-Time Tracking:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eProvides the exact location of the ISS in real time, letting users trace its current position over the Earth.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTrajectory Visualization:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eDisplays the ISS\u0026rsquo;s recent path and predicts future trajectories with stunning Earth visualizations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDay \u0026amp; Night Views:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eIncorporates real-time \u003cstrong\u003eday-night boundary visualization\u003c/strong\u003e, syncing with Earth\u0026rsquo;s rotation.\u003c/li\u003e\n\u003cli\u003eOffers an understanding of how astronauts experience 16 sunrises and sunsets daily.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAnomaly Detection:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eAnalyzes the ISS\u0026rsquo;s trajectory to predict and highlight any anomalies in its path.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInteractive Exploration:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eAllows users to explore the ISS\u0026rsquo;s majestic journey, fostering curiosity about space exploration.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"working-process\"\u003e\u003cstrong\u003eWorking Process\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Acquisition:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eRetrieves real-time satellite data using \u003cstrong\u003eSkyfield API\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Processing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eProcesses orbital elements to calculate and predict the ISS\u0026rsquo;s position and trajectory.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVisualization:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUtilizes \u003cstrong\u003eCartopy\u003c/strong\u003e for rendering Earth\u0026rsquo;s maps and the ISS\u0026rsquo;s path.\u003c/li\u003e\n\u003cli\u003eOverlays the ISS\u0026rsquo;s position, trajectory, and day-night boundaries.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAnomaly Analysis:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eImplements trajectory anomaly detection using historical data and predictions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"innovative-features\"\u003e\u003cstrong\u003eInnovative Features\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDynamic Earth Visualizations:\u003c/strong\u003e Realistic views of Earth with synchronized day-night boundaries.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHistorical Trajectory Analysis:\u003c/strong\u003e Displays the ISS\u0026rsquo;s past movements for a comprehensive context.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFuture Trajectory Predictions:\u003c/strong\u003e Predicts the ISS\u0026rsquo;s upcoming path with precision.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"tech-stack\"\u003e\u003cstrong\u003eTech Stack\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProgramming Language:\u003c/strong\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLibraries and Frameworks:\u003c/strong\u003e Skyfield, Cartopy, Matplotlib\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVisualization Tools:\u003c/strong\u003e Real-time map rendering, orbital path overlays\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTechniques:\u003c/strong\u003e Trajectory Prediction, Anomaly Detection, Data Visualization\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"applications\"\u003e\u003cstrong\u003eApplications\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSpace Exploration:\u003c/strong\u003e Real-time insights into satellite movements for enthusiasts and researchers.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEducational Tool:\u003c/strong\u003e Simplifies the complexities of orbital mechanics for learners.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAstronomy Integration:\u003c/strong\u003e Supports sky-gazing and satellite spotting for astronomy lovers.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"impact\"\u003e\u003cstrong\u003eImpact\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eThis project celebrates human curiosity and innovation by demystifying the ISS\u0026rsquo;s journey. By making satellite tracking accessible and interactive, it bridges the gap between technical data and public understanding. The next step involves integrating \u003cstrong\u003eArtificial Intelligence\u003c/strong\u003e for enhanced anomaly detection and predictive capabilities.\u003c/p\u003e","title":"Real-Time Satellite Tracking and Trajectory Analysis"},{"content":"Get in Touch Feel free to reach out to me through the following channels:\n📞 Phone: (+49 15163661969) 📧 Email: Email 🔗 LinkedIn: linkedin Looking forward to connecting with you! 🚀\n","permalink":"http://localhost:1313/Akhil_Portfolio/contact/","summary":"\u003ch1 id=\"get-in-touch\"\u003eGet in Touch\u003c/h1\u003e\n\u003cp\u003eFeel free to reach out to me through the following channels:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e📞 \u003cstrong\u003ePhone\u003c/strong\u003e:  (+49 15163661969)\u003c/li\u003e\n\u003cli\u003e📧 \u003cstrong\u003eEmail\u003c/strong\u003e: \u003ca href=\"mailto:rakhilsai95@gmail.com\"\u003eEmail\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e🔗 \u003cstrong\u003eLinkedIn\u003c/strong\u003e: \u003ca href=\"https://www.linkedin.com/in/akhil-sai-ravi-kumar-287830b5/\"\u003elinkedin\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLooking forward to connecting with you! 🚀\u003c/p\u003e","title":"Contact"},{"content":"Description The Quadcopter Live Aerial Video Surveillance System is an advanced drone platform designed to provide a live aerial video feed for real-time monitoring and analysis. This system enables rapid deployment of aerial correspondence for critical operations like search and rescue, firefighting, law enforcement, military applications, and news reporting. With future-focused capabilities such as target tracking and video compression, the system is designed for scalability and integration into various industries.\nKey Features Live Aerial Video Feed: Captures and transmits real-time video from the drone to a remote controller or computer. Digital Video Processing: Converts captured video signals into digital format for analysis and storage. Remote and Voice Control: Operates the drone with remote control, with future expansions for voice control integration. Auto-Landing: Ensures safe and precise landing using flight control systems. Target Tracking and Video Compression (Future Expansion): Potential to integrate advanced algorithms for target identification and bandwidth-efficient video transmission. Components Quadcopter/Controller System: Frame: Durable structure to support all hardware components. DC Motors and Rotors: Provide propulsion and maneuverability. Power Supply: Battery-driven system for sustained operation. Flight Control Board: Manages stability and flight dynamics. Flight Control Software: Ensures smooth operation and maneuvering. RC Controller: Enables precise remote navigation. Video Transmission System: Camera: Captures high-resolution aerial footage. Transmitter and Receiver: Streams live video feed to the ground control station. Display: Visualizes the live feed in real-time on a monitor or computer. Digital Video Analysis System: Signal Processing: Converts raw camera signals into a digital format. Video Output: Displays the processed feed on connected devices. Applications Search and Rescue: Rapid deployment in disaster-hit areas to locate survivors and assess damage. Firefighting: Real-time aerial monitoring to track fire spread and aid firefighting strategies. Law Enforcement: Surveillance and monitoring for public safety and criminal investigations. Military: Tactical reconnaissance and monitoring in high-risk zones. News Reporting: Live aerial coverage of events, enabling faster reporting. Working Process Data Acquisition: Captures video feed via a mounted camera on the quadcopter. Data Transmission: Transfers the video signal to the receiver using a transmitter. Signal Conversion: Converts the received signal into a digital video format. Video Analysis: Analyzes and stores video data for further use, with future provisions for tracking and compression. Tech Stack Hardware: Quadcopter frame, DC motors, rotors, flight control system, and RC controller. Software: Digital signal processing, flight control software. Video Transmission: Camera and transmitter/receiver modules. Impact This project bridges the gap between traditional surveillance systems and modern drone technology. It provides a faster, cost-effective, and versatile alternative to conventional helicopters, making it an indispensable tool in emergency response and monitoring scenarios. The future integration of sentience, target tracking, and video compression opens avenues for more advanced applications.\n","permalink":"http://localhost:1313/Akhil_Portfolio/projects/drone/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eQuadcopter Live Aerial Video Surveillance System\u003c/strong\u003e is an advanced drone platform designed to provide a live aerial video feed for real-time monitoring and analysis. This system enables rapid deployment of aerial correspondence for critical operations like \u003cstrong\u003esearch and rescue, firefighting, law enforcement, military applications\u003c/strong\u003e, and \u003cstrong\u003enews reporting\u003c/strong\u003e. With future-focused capabilities such as \u003cstrong\u003etarget tracking\u003c/strong\u003e and \u003cstrong\u003evideo compression\u003c/strong\u003e, the system is designed for scalability and integration into various industries.\u003c/p\u003e","title":"Quadcopter Live Aerial Video Surveillance System"},{"content":"Description As a Software Developer and Research Engineer specializing inGenerative AI, AI, and Machine Learning at ASTRIAL GmbH, I have spearheaded the development of a co-pilot designed to analyze security alerts, reduce false positives, and prioritize alarms, streamlining workflows for less-experienced security personnel.\nMy responsibilities included extensive TTP data preprocessing, where I rephrased and shortened sentences to preserve their original meaning, enhancing model comprehension. Leveraging SecureBERT, I handled domain-specific terms to improve the detection and understanding of complex technical concepts in cybersecurity data. Additionally, I utilized GPT-3.5 to rephrase sentences, ensuring readability, diversity, and clarity while maintaining their core intent.\nTo enable intuitive threat analysis, I integrated preprocessed TTP data into Neo4j, visualizing TTPs and SubTTPs along with their relationships. Further, I incorporated Llama2 with the Neo4j GraphDatabase, empowering the co-pilot to guide users in navigating complex security datasets and identifying techniques and tactics effectively.\nMy expertise spans deep learning models such as BERT, GPT, T5, and OpenAI APIs, applied to tasks like text classification, summarization, and question-answering. I have fine-tuned LLMs (e.g., Llama2, GPT) for domain-specific applications, applied prompt engineering to optimize LLM outputs, and implemented cosine similarity and vector embeddings for tasks like Named Entity Recognition (NER) and semantic search.\nIn addition to my NLP expertise, I have delivered end-to-end image processing solutions, including data acquisition, preprocessing, model training, and deployment. I enhanced object detection and segmentation algorithms using tools like YoloV5, OpenCV, and SAM (Segment Anything Model) while optimizing models for edge device deployments with ONNX and OpenVINO.\nFurther accomplishments include designing sound detection algorithms using FFT and geo-localization techniques, applying advanced audio preprocessing for signal quality optimization, and deploying applications with Docker-based CI/CD pipelines for scalable, reliable delivery. My focus on model robustness and data optimization techniques has ensured consistent, high-quality outputs.\nI also contributed to team development by mentoring interns and master’s students, fostering technical excellence and collaborative learning. Through a combination of technical innovation and mentorship, I have driven impactful solutions in AI-powered cybersecurity, computer vision, and generative AI.\n","permalink":"http://localhost:1313/Akhil_Portfolio/experience/astrial/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs a \u003cstrong\u003eSoftware Developer and Research Engineer\u003c/strong\u003e specializing in\u003cstrong\u003eGenerative AI, AI, and Machine Learning\u003c/strong\u003e at \u003cstrong\u003eASTRIAL GmbH\u003c/strong\u003e, I have spearheaded the development of a \u003cstrong\u003eco-pilot\u003c/strong\u003e designed to analyze security alerts, reduce false positives, and prioritize alarms, streamlining workflows for less-experienced security personnel.\u003c/p\u003e\n\u003cp\u003eMy responsibilities included extensive TTP data preprocessing, where I rephrased and shortened sentences to preserve their original meaning, enhancing model comprehension. Leveraging \u003cstrong\u003eSecureBERT\u003c/strong\u003e, I handled domain-specific terms to improve the detection and understanding of complex technical concepts in \u003cstrong\u003ecybersecurity\u003c/strong\u003e data. Additionally, I utilized \u003cstrong\u003eGPT-3.5\u003c/strong\u003e to rephrase sentences, ensuring readability, diversity, and clarity while maintaining their core intent.\u003c/p\u003e","title":"Senior Software Developer and Research engineer"},{"content":"Description As a Python Developer specializing in Simulation Frameworks, UI Development, and Machine Learning, I contributed to building advanced systems for demand-driven environments and predictive modeling. My work focused on enhancing supply chain management tools, developing user-friendly interfaces, and creating machine learning models to improve forecasting and decision-making processes.\nI designed and implemented an improved User Interface (UI) for DDMRP (Demand-Driven Material Requirements Planning) and MRP (Material Requirements Planning) engines, optimizing usability and operational efficiency in supply chain systems. Leveraging Flask, I built a dynamic web application to display, edit, and manage input values for simulation engines, including WMAPE, DDMRP, and MRP, enabling real-time updates and streamlined data handling.\nTo standardize and accelerate development, I developed custom reusable User Controls in Flask, ensuring consistent and efficient design across multiple screens. I also architected key components such as the Data Access Layer (DAL) and Business Logic Layer (BLL), ensuring robust backend architecture and seamless data flow.\nIn the domain of security and authentication, I implemented secure user sign-in and sign-up mechanisms, integrating third-party authentication providers like Google and Microsoft to enhance application security. Additionally, I developed a WMAPE automatic error identification system, incorporating heatmap graphs to provide intuitive visual diagnostics for demand generator engines.\nI also contributed to machine learning model development, building models for time series data generation and future demand prediction. By utilizing algorithms such as Random Forest, Isolation Forest, and LSTM neural networks, I improved forecasting accuracy and reliability in dynamic demand environments.\nThrough performance optimization and troubleshooting, I ensured seamless operation and an enhanced user experience across all modules. My technical expertise in UI development, backend architecture, and machine learning enabled me to deliver impactful solutions for simulation frameworks and demand-driven applications.\n","permalink":"http://localhost:1313/Akhil_Portfolio/experience/camelot/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs a \u003cstrong\u003ePython Developer\u003c/strong\u003e specializing in Simulation \u003cstrong\u003eFrameworks, UI Development\u003c/strong\u003e, and \u003cstrong\u003eMachine Learning\u003c/strong\u003e, I contributed to building advanced systems for demand-driven environments and predictive modeling. My work focused on enhancing supply chain management tools, developing user-friendly interfaces, and creating machine learning models to improve forecasting and decision-making processes.\u003c/p\u003e\n\u003cp\u003eI designed and implemented an improved User Interface (\u003cstrong\u003eUI\u003c/strong\u003e) for \u003cstrong\u003eDDMRP\u003c/strong\u003e (Demand-Driven Material Requirements Planning) and \u003cstrong\u003eMRP\u003c/strong\u003e (Material Requirements Planning) engines, optimizing usability and operational efficiency in supply chain systems. Leveraging \u003cstrong\u003eFlask\u003c/strong\u003e, I built a dynamic web application to display, edit, and manage input values for simulation engines, including \u003cstrong\u003eWMAPE, DDMRP, and MRP, enabling real-time updates\u003c/strong\u003e and \u003cstrong\u003estreamlined data handling\u003c/strong\u003e.\u003c/p\u003e","title":"Python Developer (Intern)"},{"content":"Description As a Software Design and Developer Engineer at Tech Mahindra LTD, I worked extensively on designing, developing, and deploying software solutions leveraging technologies like Python, AWS, .NET, HTML, CSS, JavaScript, and Microsoft Azure Cloud. My role involved automation, responsive web design, cloud infrastructure management, and enhancing application performance for seamless operations.\nI automated Azure resource creation using PowerShell scripts, managing resource groups, web applications, storage blobs/tables, and firewall rules to optimize cloud deployment processes. Additionally, I developed responsive web designs (RWD) using Media Queries, Bootstrap, HTML, CSS, and JavaScript, ensuring cross-device compatibility and delivering consistent user experiences across platforms.\nIn application development and deployment, I configured and managed IIS Web Servers for hosting and collaborated in system specification meetings to align development with user requirements. I designed and implemented CI/CD pipelines using AWS CodePipeline, CodeCommit, and CodeDeploy, streamlining continuous integration and delivery for efficient software deployment.\nMy expertise extended to cloud infrastructure management, where I deployed and maintained Azure IaaS virtual machines (VMs), PaaS role instances, and AWS Elastic Beanstalk environments, ensuring scalable and reliable infrastructure. I also managed version control systems using GIT, GitHub, and TFS, employing structured branching strategies to maintain codebase integrity and foster collaboration.\nAs part of technical troubleshooting, I diagnosed and resolved application performance issues, verified data integrity, and identified code defects. I implemented SSO policies and federated identities, managing dynamic group memberships and user access policies across multiple SaaS applications to enhance security and identity management.\nFurthermore, I collaborated on database functionalities, data analytics, and reporting requirements, ensuring alignment with organizational objectives. My comprehensive skill set in software development, cloud automation, and infrastructure management enabled me to deliver high-impact solutions for enterprise-grade applications.\n","permalink":"http://localhost:1313/Akhil_Portfolio/experience/tech-mahindra-ltd/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs a Software Design and Developer Engineer at \u003cstrong\u003eTech Mahindra LTD\u003c/strong\u003e, I worked extensively on designing, developing, and deploying software solutions leveraging technologies like Python, AWS, .NET, HTML, CSS, JavaScript, and Microsoft Azure Cloud. My role involved \u003cstrong\u003eautomation, responsive web design, cloud infrastructure management\u003c/strong\u003e, and \u003cstrong\u003eenhancing application performance\u003c/strong\u003e for seamless operations.\u003c/p\u003e\n\u003cp\u003eI automated Azure resource creation using PowerShell scripts, managing resource groups, web applications, storage blobs/tables, and firewall rules to optimize cloud deployment processes. Additionally, I developed responsive web designs (\u003cstrong\u003eRWD\u003c/strong\u003e) using \u003cstrong\u003eMedia Queries, Bootstrap, HTML, CSS\u003c/strong\u003e, and \u003cstrong\u003eJavaScript\u003c/strong\u003e, ensuring cross-device compatibility and delivering consistent user experiences across platforms.\u003c/p\u003e","title":"Software Design and Developer Engineer"},{"content":"Description As a Junior Software Developer (Intern) specializing in AI, ML, and Robotics, I played a pivotal role in designing and developing a prototype smart rover named as MOWME, leveraging AI, ML, and DL technologies to enable autonomous operations. The rover was equipped with SLAM (Simultaneous Localization and Mapping) capabilities and utilized CNN networks for task identification and classification. I integrated advanced features such as object detection, obstacle avoidance, GPS waypoint generation, and self-testing mechanisms, achieving seamless autonomous navigation.\nTo ensure high-quality datasets, I built automated data preprocessing pipelines, incorporating steps like Normalization, Standardization, Resizing, Denoising, Segmentation, and Morphology, as well as JSON creation and Data Pickling for efficient data handling. These pipelines enabled consistent and reliable input data for machine learning workflows.\nIn the domain of Machine Learning and Model Optimization, I implemented Regression, Classification, and Ensemble algorithms, troubleshooting bottlenecks to enhance model accuracy and reduce inference latency. Optimized models were deployed on edge devices such as Raspberry Pi 3, Jetson Nano, and AWS EC2 GPU instances, leveraging GPU acceleration with tools like CUDA and cuDNN for efficient performance.\nAdditionally, I integrated IoT devices like TCG4 sensors and Beacon devices to collect metrics (e.g., acceleration, pressure, temperature, humidity) and established cloud communication pipelines with AWS. I worked with advanced communication protocols including SOMEIP, VSOMEIP, and CAN to ensure robust and stable device connectivity.\nMy contributions also included designing and developing a web-based AI model development platform, enabling users to build image-based AI models for object detection, classification, and recognition without requiring programming expertise.\nThrough effective CI/CD pipeline management and version control systems like GitHub and GitLab, I ensured streamlined workflows, reliable deployments, and system scalability. My work combined technical expertise and innovation to deliver a versatile smart rover and empower users with accessible AI-driven solutions.\n","permalink":"http://localhost:1313/Akhil_Portfolio/experience/avl-software-and-functions-gmbh/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eAs a \u003cstrong\u003eJunior Software Developer\u003c/strong\u003e (Intern) specializing in \u003cstrong\u003eAI, ML\u003c/strong\u003e, and \u003cstrong\u003eRobotics\u003c/strong\u003e, I played a pivotal role in designing and developing a prototype \u003cstrong\u003esmart rover\u003c/strong\u003e named as \u003cstrong\u003eMOWME\u003c/strong\u003e, leveraging AI, ML, and DL technologies to enable autonomous operations. The rover was equipped with \u003cstrong\u003eSLAM\u003c/strong\u003e (Simultaneous Localization and Mapping) capabilities and utilized CNN networks for task identification and classification. I integrated advanced features such as \u003cstrong\u003eobject detection\u003c/strong\u003e, \u003cstrong\u003eobstacle avoidance\u003c/strong\u003e, \u003cstrong\u003eGPS\u003c/strong\u003e waypoint generation, and self-testing mechanisms, achieving seamless autonomous navigation.\u003c/p\u003e","title":"Junior Software Developer (Intern)"},{"content":"Description This Web-Based CNN Model Tester and Trainer is an intuitive application designed to simplify AI model development for image-based tasks like object detection, classification, and recognition. The platform enables users to create, train, and test AI models without requiring expertise in programming or machine learning.\nKey Features Automated Data Preprocessing: Handles cleaning, augmentation, and preparation of image datasets. End-to-End Model Training: Trains Convolutional Neural Network (CNN) models using uploaded datasets. Model Testing: Allows users to upload images or capture them in real time for testing model predictions. Model Deployment: Provides the trained model in formats like H5, JSON, ready for deployment on edge devices. Real-Time Dataset Capture: Supports capturing datasets directly via webcam or connected cameras. User-Friendly Workflow: Offers a no-code experience for AI enthusiasts, researchers, and developers. Working Process Data Input: Users can upload datasets or capture real-time images for training and testing. Data Preprocessing: The tool cleans and prepares the data automatically, ensuring high-quality inputs. Model Training: CNN models are trained using TensorFlow, OpenVINO, or ONNX. Model Testing: Users can test the model on uploaded images or real-time captures. Model Export: Trained models are made available in formats such as H5, JSON, and ONNX for deployment. Applications Autonomous Vehicle Development: Used by AVL Software and Functions GmbH for ADAS (Advanced Driver Assistance Systems) and autonomous vehicle projects. AI Learning and Research: Makes AI accessible for researchers and enthusiasts by eliminating the need for advanced programming skills. Custom AI Solutions: Suitable for creating tailor-made AI models for object detection and recognition. Tech Stack Programming Languages: Python, HTML, CSS, JavaScript. Frameworks and Libraries: TensorFlow, Keras, OpenCV, TensorflowJS, Flask, FAST API. Tools and Technologies: Protobuf, OpenVINO, ONNX, TensorBoard, CUDA, cuDNN. Neural Network: Convolutional Neural Network (CNN). Impact The primary aim of this project is to make AI model development easy, quick, and accessible, empowering users with limited technical knowledge to develop AI solutions independently. The platform’s deployment at AVL Software and Functions GmbH demonstrates its effectiveness in supporting ADAS and autonomous vehicle projects.\n","permalink":"http://localhost:1313/Akhil_Portfolio/projects/master_thesis/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThis \u003cstrong\u003eWeb-Based CNN Model Tester and Trainer\u003c/strong\u003e is an intuitive application designed to simplify AI model development for image-based tasks like \u003cstrong\u003eobject detection, classification, and recognition\u003c/strong\u003e. The platform enables users to create, train, and test AI models without requiring expertise in programming or machine learning.\u003c/p\u003e\n\u003ch3 id=\"key-features\"\u003e\u003cstrong\u003eKey Features\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eAutomated Data Preprocessing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eHandles cleaning, augmentation, and preparation of image datasets.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEnd-to-End Model Training:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eTrains Convolutional Neural Network (CNN) models using uploaded datasets.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel Testing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eAllows users to upload images or capture them in real time for testing model predictions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel Deployment:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eProvides the trained model in formats like \u003cstrong\u003eH5, JSON\u003c/strong\u003e, ready for deployment on edge devices.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReal-Time Dataset Capture:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eSupports capturing datasets directly via webcam or connected cameras.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUser-Friendly Workflow:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eOffers a no-code experience for AI enthusiasts, researchers, and developers.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"working-process\"\u003e\u003cstrong\u003eWorking Process\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Input:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUsers can upload datasets or capture real-time images for training and testing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Preprocessing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eThe tool cleans and prepares the data automatically, ensuring high-quality inputs.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel Training:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eCNN models are trained using TensorFlow, OpenVINO, or ONNX.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel Testing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUsers can test the model on uploaded images or real-time captures.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel Export:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eTrained models are made available in formats such as H5, JSON, and ONNX for deployment.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"applications\"\u003e\u003cstrong\u003eApplications\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAutonomous Vehicle Development:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUsed by AVL Software and Functions GmbH for ADAS (Advanced Driver Assistance Systems) and autonomous vehicle projects.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAI Learning and Research:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eMakes AI accessible for researchers and enthusiasts by eliminating the need for advanced programming skills.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCustom AI Solutions:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eSuitable for creating tailor-made AI models for object detection and recognition.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"tech-stack\"\u003e\u003cstrong\u003eTech Stack\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProgramming Languages:\u003c/strong\u003e Python, HTML, CSS, JavaScript.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFrameworks and Libraries:\u003c/strong\u003e TensorFlow, Keras, OpenCV, TensorflowJS, Flask, FAST API.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTools and Technologies:\u003c/strong\u003e Protobuf, OpenVINO, ONNX, TensorBoard, CUDA, cuDNN.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNeural Network:\u003c/strong\u003e Convolutional Neural Network (CNN).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"impact\"\u003e\u003cstrong\u003eImpact\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eThe primary aim of this project is to make AI model development \u003cstrong\u003eeasy, quick, and accessible\u003c/strong\u003e, empowering users with limited technical knowledge to develop AI solutions independently. The platform’s deployment at AVL Software and Functions GmbH demonstrates its effectiveness in supporting \u003cstrong\u003eADAS and autonomous vehicle projects\u003c/strong\u003e.\u003c/p\u003e","title":"Web-Based CNN Model Tester and Trainer"},{"content":"Description Project MOWME is an innovative Autonomous Driving Rover prototype designed to showcase advanced AI, ML, and IoT-driven capabilities. The rover is built to autonomously navigate, classify objects, collect and self-train data, and transfer insights to the cloud for future learning. This system integrates advanced hardware with cloud storage and machine learning for real-time decision-making and environmental analysis.\nKey Features Object Classification: The rover uses machine learning to identify and classify objects in its environment. Auto-Data Collection: Captures real-time sensor and camera data, processing it locally or storing it in the cloud. Self-Training: Implements feedback loops to refine object classification and navigation algorithms over time. Auto-Navigation: Incorporates GPS waypoint navigation for autonomous movement across dynamic terrains. Obstacle Avoidance: Uses ultrasonic sensors for collision detection and avoidance. Cloud Integration: Data is stored on AWS Cloud, enabling future training and analysis. Environmental Monitoring: Detects diseases in grass using Convolutional Neural Networks (CNN), identifying patterns like dryness or overwatering. Hardware Components Core Components: Raspberry Pi 4 Arduino Raspberry Pi Camera L298D Motor Drive Additional Sensors: Ultrasonic Sensors GPS Module Bluetooth Module Software Features Wireless Drive Control: The rover can be controlled using MQTT protocol for wireless operation. Video Streaming: Real-time video streaming from Raspberry Pi to a laptop for monitoring. Dataset Collection: Automatically captures and processes images for model training, storing them locally or in the cloud. Disease Detection in Grass: Leverages CNN models to analyze grass conditions and classify diseases. Working Process Data Collection: Captures sensor data, camera images, and ultrasonic measurements. Transfers data to AWS Cloud for processing and storage. Data Processing: Object classification and dataset analysis using TensorFlow and CNN models. Disease detection in grass based on visual patterns. Navigation and Control: Utilizes GPS and sensor data for waypoint navigation. Avoids obstacles dynamically with ultrasonic sensors. Wireless Communication: MQTT protocol enables seamless communication between devices. Live monitoring of rover operations via Raspberry Pi. Programming Languages Python: (80%) Core logic, machine learning models, and data processing. C++: (20%) For low-level hardware interaction and control. AWS Integration: IoT and cloud-based storage for scalability. Applications Autonomous lawn management systems. Environmental monitoring and data collection. IoT-driven robotics for agriculture. AI-based disease detection and analytics. The MOWME Rover demonstrates the seamless integration of hardware, AI, and cloud services, offering a versatile solution for autonomous systems and environmental analysis. This project is a step forward in creating smart, scalable, and adaptive IoT-enabled devices for practical use cases.\n","permalink":"http://localhost:1313/Akhil_Portfolio/projects/mowme/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eProject \u003cstrong\u003eMOWME\u003c/strong\u003e is an innovative \u003cstrong\u003eAutonomous Driving Rover\u003c/strong\u003e prototype designed to showcase advanced AI, ML, and IoT-driven capabilities. The rover is built to autonomously navigate, classify objects, collect and self-train data, and transfer insights to the cloud for future learning. This system integrates advanced hardware with cloud storage and machine learning for real-time decision-making and environmental analysis.\u003c/p\u003e\n\u003ch3 id=\"key-features\"\u003e\u003cstrong\u003eKey Features\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eObject Classification:\u003c/strong\u003e The rover uses machine learning to identify and classify objects in its environment.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAuto-Data Collection:\u003c/strong\u003e Captures real-time sensor and camera data, processing it locally or storing it in the cloud.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Training:\u003c/strong\u003e Implements feedback loops to refine object classification and navigation algorithms over time.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAuto-Navigation:\u003c/strong\u003e Incorporates \u003cstrong\u003eGPS waypoint navigation\u003c/strong\u003e for autonomous movement across dynamic terrains.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eObstacle Avoidance:\u003c/strong\u003e Uses ultrasonic sensors for collision detection and avoidance.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCloud Integration:\u003c/strong\u003e Data is stored on \u003cstrong\u003eAWS Cloud\u003c/strong\u003e, enabling future training and analysis.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEnvironmental Monitoring:\u003c/strong\u003e Detects diseases in grass using \u003cstrong\u003eConvolutional Neural Networks (CNN)\u003c/strong\u003e, identifying patterns like dryness or overwatering.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"hardware-components\"\u003e\u003cstrong\u003eHardware Components\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCore Components:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eRaspberry Pi 4\u003c/li\u003e\n\u003cli\u003eArduino\u003c/li\u003e\n\u003cli\u003eRaspberry Pi Camera\u003c/li\u003e\n\u003cli\u003eL298D Motor Drive\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAdditional Sensors:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUltrasonic Sensors\u003c/li\u003e\n\u003cli\u003eGPS Module\u003c/li\u003e\n\u003cli\u003eBluetooth Module\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"software-features\"\u003e\u003cstrong\u003eSoftware Features\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eWireless Drive Control:\u003c/strong\u003e The rover can be controlled using MQTT protocol for wireless operation.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVideo Streaming:\u003c/strong\u003e Real-time video streaming from Raspberry Pi to a laptop for monitoring.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDataset Collection:\u003c/strong\u003e Automatically captures and processes images for model training, storing them locally or in the cloud.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDisease Detection in Grass:\u003c/strong\u003e Leverages CNN models to analyze grass conditions and classify diseases.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"working-process\"\u003e\u003cstrong\u003eWorking Process\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Collection:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eCaptures sensor data, camera images, and ultrasonic measurements.\u003c/li\u003e\n\u003cli\u003eTransfers data to AWS Cloud for processing and storage.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Processing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eObject classification and dataset analysis using \u003cstrong\u003eTensorFlow\u003c/strong\u003e and \u003cstrong\u003eCNN models\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eDisease detection in grass based on visual patterns.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNavigation and Control:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUtilizes GPS and sensor data for waypoint navigation.\u003c/li\u003e\n\u003cli\u003eAvoids obstacles dynamically with ultrasonic sensors.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWireless Communication:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eMQTT protocol enables seamless communication between devices.\u003c/li\u003e\n\u003cli\u003eLive monitoring of rover operations via Raspberry Pi.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"programming-languages\"\u003e\u003cstrong\u003eProgramming Languages\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePython:\u003c/strong\u003e (80%) Core logic, machine learning models, and data processing.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eC++:\u003c/strong\u003e (20%) For low-level hardware interaction and control.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAWS Integration:\u003c/strong\u003e IoT and cloud-based storage for scalability.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"applications\"\u003e\u003cstrong\u003eApplications\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAutonomous lawn management systems.\u003c/li\u003e\n\u003cli\u003eEnvironmental monitoring and data collection.\u003c/li\u003e\n\u003cli\u003eIoT-driven robotics for agriculture.\u003c/li\u003e\n\u003cli\u003eAI-based disease detection and analytics.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003cstrong\u003eMOWME Rover\u003c/strong\u003e demonstrates the seamless integration of hardware, AI, and cloud services, offering a versatile solution for autonomous systems and environmental analysis. This project is a step forward in creating smart, scalable, and adaptive IoT-enabled devices for practical use cases.\u003c/p\u003e","title":"Autonomous Driving Rover: Project MOWME"},{"content":"Description This project focuses on developing an intelligent Assets and Alerts Copilot system that leverages graph-based data modeling and AI-powered insights to provide a comprehensive cybersecurity solution. The system integrates advanced tools like Neo4j, Mistral, and Streamlit to enable real-time asset and alert detection, relationship mapping, and contextual analysis.\nThe graph-based data modeling ensures accurate visualization of assets, products, vulnerabilities, and their relationships, providing unparalleled clarity in understanding interconnected cybersecurity risks. With the Mistral LLM, the copilot allows users to interact using natural language queries, transforming complex data into actionable insights.\nThe system provides real-time responses to queries, dynamic alert prioritization, and customizable visualizations of vulnerabilities and asset connections. It is particularly valuable for security teams seeking to reduce alert fatigue, understand critical relationships, and prioritize actionable insights.\nThis project represents a cutting-edge approach to cybersecurity operations, combining advanced AI with graph-driven analysis to enhance threat detection, incident response, and asset management.\nTechnologies Used Programming Languages: Python Libraries and Frameworks: GCP Functions, Neo4J, Streamlit Applications: IT security team ","permalink":"http://localhost:1313/Akhil_Portfolio/projects/assets-and-alerts/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThis project focuses on developing an \u003cstrong\u003eintelligent Assets and Alerts Copilot system\u003c/strong\u003e that leverages graph-based data modeling and AI-powered insights to provide a comprehensive cybersecurity solution. The system integrates advanced tools like \u003cstrong\u003eNeo4j\u003c/strong\u003e, \u003cstrong\u003eMistral\u003c/strong\u003e, and \u003cstrong\u003eStreamlit\u003c/strong\u003e to enable real-time asset and alert detection, relationship mapping, and contextual analysis.\u003c/p\u003e\n\u003cp\u003eThe \u003cstrong\u003egraph-based data modeling\u003c/strong\u003e ensures accurate visualization of assets, products, vulnerabilities, and their relationships, providing unparalleled clarity in understanding interconnected cybersecurity risks. With the \u003cstrong\u003eMistral LLM\u003c/strong\u003e, the copilot allows users to interact using natural language queries, transforming complex data into actionable insights.\u003c/p\u003e","title":"Assets and Alerts Copilot Using Graph-Based AI Systems"},{"content":"Description This project showcases the development of an Autonomous Vehicle System (AVS) prototype, integrating advanced AI, Machine Learning (ML), and Deep Learning (DL) techniques with embedded systems. The project leverages a combination of Raspberry Pi 4, Arduino, and NVIDIA Jetson Nano/TX2 NX, along with additional hardware components, to create a scalable and efficient system that simulates real-world autonomous vehicle capabilities.\nKey Features: Self-driving: Autonomous navigation with path following. Obstacle avoidance: Detection and navigation around obstacles using ultrasonic sensors and LiDAR/Radar. Self-parking: Automated parking into predefined spaces. Traffic signboard recognition: Classification of traffic signs using machine learning models. Voice command recognition: Hands-free vehicle control through voice inputs. Waypoint navigation: GPS-based precise navigation to specific destinations. Real-time data collection: Integration of telemetry and sensor data for system optimization. Hardware Components: Primary Hardware: Raspberry Pi 4 Arduino Raspberry Pi Camera with Night Vision Sensors L298D Motor Drive Raspberry Pi Microphone and Speaker Additional Hardware: Ultrasonic Sensors LiDAR/Radar GPS Module Bluetooth Module Concepts and Technologies: Computer Vision: Object detection, sign recognition, and real-time image processing using OpenCV. Machine Learning and Deep Learning: TensorFlow and PyTorch-based models for sign classification and voice recognition. GPS Navigation: Waypoint-based navigation for precise route planning. Obstacle Avoidance: Real-time distance measurement and navigation using ultrasonic sensors and LiDAR. Auto Parking: Automated parking algorithms for efficient maneuvering. Traffic Signboard Recognition: AI-driven recognition of traffic signs to simulate real-world scenarios. Programming Languages: Python: (80%) Used for most AI, ML, and sensor data processing tasks. C++: (20%) For low-level hardware integration and control. Java and CSS: For GPS tracking through mobile and smartwatch applications Working Process: Data Input: Real-time data collection from ultrasonic sensors, LiDAR, and cameras. Data Processing: Object detection and obstacle avoidance using OpenCV and machine learning models. Traffic sign recognition and decision-making with TensorFlow 2.0. Vehicle Control: Autonomous navigation and parking using processed sensor data. Output and Monitoring: Real-time visualization and monitoring via a local server (localhost). Applications: Autonomous driving research and development. Robotics and intelligent automation. Smart transportation systems. AI-based real-time monitoring and data collection. The Autonomous Driving Prototype combines cutting-edge AI techniques with practical embedded systems to offer a versatile and scalable solution for autonomous navigation and control. This project demonstrates how emerging technologies can be used to bridge the gap between AI research and real-world applications.\n","permalink":"http://localhost:1313/Akhil_Portfolio/projects/autonomus_vehicle/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThis project showcases the development of an \u003cstrong\u003eAutonomous Vehicle System (AVS) prototype\u003c/strong\u003e, integrating advanced \u003cstrong\u003eAI, Machine Learning (ML), and Deep Learning (DL)\u003c/strong\u003e techniques with embedded systems. The project leverages a combination of \u003cstrong\u003eRaspberry Pi 4\u003c/strong\u003e, \u003cstrong\u003eArduino\u003c/strong\u003e, and \u003cstrong\u003eNVIDIA Jetson Nano/TX2 NX\u003c/strong\u003e, along with additional hardware components, to create a scalable and efficient system that simulates real-world autonomous vehicle capabilities.\u003c/p\u003e\n\u003ch3 id=\"key-features\"\u003e\u003cstrong\u003eKey Features:\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-driving:\u003c/strong\u003e Autonomous navigation with path following.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eObstacle avoidance:\u003c/strong\u003e Detection and navigation around obstacles using ultrasonic sensors and LiDAR/Radar.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-parking:\u003c/strong\u003e Automated parking into predefined spaces.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraffic signboard recognition:\u003c/strong\u003e Classification of traffic signs using machine learning models.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVoice command recognition:\u003c/strong\u003e Hands-free vehicle control through voice inputs.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWaypoint navigation:\u003c/strong\u003e GPS-based precise navigation to specific destinations.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time data collection:\u003c/strong\u003e Integration of telemetry and sensor data for system optimization.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-components\"\u003e\u003cstrong\u003eHardware Components:\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePrimary Hardware:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eRaspberry Pi 4\u003c/li\u003e\n\u003cli\u003eArduino\u003c/li\u003e\n\u003cli\u003eRaspberry Pi Camera with Night Vision Sensors\u003c/li\u003e\n\u003cli\u003eL298D Motor Drive\u003c/li\u003e\n\u003cli\u003eRaspberry Pi Microphone and Speaker\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAdditional Hardware:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUltrasonic Sensors\u003c/li\u003e\n\u003cli\u003eLiDAR/Radar\u003c/li\u003e\n\u003cli\u003eGPS Module\u003c/li\u003e\n\u003cli\u003eBluetooth Module\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"concepts-and-technologies\"\u003e\u003cstrong\u003eConcepts and Technologies:\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eComputer Vision:\u003c/strong\u003e Object detection, sign recognition, and real-time image processing using \u003cstrong\u003eOpenCV\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMachine Learning and Deep Learning:\u003c/strong\u003e TensorFlow and PyTorch-based models for sign classification and voice recognition.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGPS Navigation:\u003c/strong\u003e Waypoint-based navigation for precise route planning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eObstacle Avoidance:\u003c/strong\u003e Real-time distance measurement and navigation using ultrasonic sensors and LiDAR.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAuto Parking:\u003c/strong\u003e Automated parking algorithms for efficient maneuvering.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraffic Signboard Recognition:\u003c/strong\u003e AI-driven recognition of traffic signs to simulate real-world scenarios.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"programming-languages\"\u003e\u003cstrong\u003eProgramming Languages:\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePython:\u003c/strong\u003e (80%) Used for most AI, ML, and sensor data processing tasks.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eC++:\u003c/strong\u003e (20%) For low-level hardware integration and control.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eJava and CSS:\u003c/strong\u003e For GPS tracking through mobile and smartwatch applications\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"working-process\"\u003e\u003cstrong\u003eWorking Process:\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Input:\u003c/strong\u003e Real-time data collection from ultrasonic sensors, LiDAR, and cameras.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Processing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eObject detection and obstacle avoidance using OpenCV and machine learning models.\u003c/li\u003e\n\u003cli\u003eTraffic sign recognition and decision-making with TensorFlow 2.0.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVehicle Control:\u003c/strong\u003e Autonomous navigation and parking using processed sensor data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOutput and Monitoring:\u003c/strong\u003e Real-time visualization and monitoring via a local server (localhost).\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"applications\"\u003e\u003cstrong\u003eApplications:\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAutonomous driving research and development.\u003c/li\u003e\n\u003cli\u003eRobotics and intelligent automation.\u003c/li\u003e\n\u003cli\u003eSmart transportation systems.\u003c/li\u003e\n\u003cli\u003eAI-based real-time monitoring and data collection.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003cstrong\u003eAutonomous Driving Prototype\u003c/strong\u003e combines cutting-edge AI techniques with practical embedded systems to offer a versatile and scalable solution for autonomous navigation and control. This project demonstrates how emerging technologies can be used to bridge the gap between AI research and real-world applications.\u003c/p\u003e","title":"Autonomous Driving Prototype Using Embedded Systems and AI"},{"content":"Description This Real-Time Satellite Tracking and Trajectory Analysis project is a comprehensive system designed to monitor the International Space Station (ISS) in real time, visualize its trajectory, and analyze its past and future paths. The project offers interactive visualizations, anomaly detection capabilities, and insights into the ISS\u0026rsquo;s movement across Earth.\nKey Features Real-Time Tracking: Provides the exact location of the ISS in real time, letting users trace its current position over the Earth. Trajectory Visualization: Displays the ISS\u0026rsquo;s recent path and predicts future trajectories with stunning Earth visualizations. Day \u0026amp; Night Views: Incorporates real-time day-night boundary visualization, syncing with Earth\u0026rsquo;s rotation. Offers an understanding of how astronauts experience 16 sunrises and sunsets daily. Anomaly Detection: Analyzes the ISS\u0026rsquo;s trajectory to predict and highlight any anomalies in its path. Interactive Exploration: Allows users to explore the ISS\u0026rsquo;s majestic journey, fostering curiosity about space exploration. Working Process Data Acquisition: Retrieves real-time satellite data using Skyfield API. Data Processing: Processes orbital elements to calculate and predict the ISS\u0026rsquo;s position and trajectory. Visualization: Utilizes Cartopy for rendering Earth\u0026rsquo;s maps and the ISS\u0026rsquo;s path. Overlays the ISS\u0026rsquo;s position, trajectory, and day-night boundaries. Anomaly Analysis: Implements trajectory anomaly detection using historical data and predictions. Innovative Features Dynamic Earth Visualizations: Realistic views of Earth with synchronized day-night boundaries. Historical Trajectory Analysis: Displays the ISS\u0026rsquo;s past movements for a comprehensive context. Future Trajectory Predictions: Predicts the ISS\u0026rsquo;s upcoming path with precision. Tech Stack Programming Language: Python Libraries and Frameworks: Skyfield, Cartopy, Matplotlib Visualization Tools: Real-time map rendering, orbital path overlays Techniques: Trajectory Prediction, Anomaly Detection, Data Visualization Applications Space Exploration: Real-time insights into satellite movements for enthusiasts and researchers. Educational Tool: Simplifies the complexities of orbital mechanics for learners. Astronomy Integration: Supports sky-gazing and satellite spotting for astronomy lovers. Impact This project celebrates human curiosity and innovation by demystifying the ISS\u0026rsquo;s journey. By making satellite tracking accessible and interactive, it bridges the gap between technical data and public understanding. The next step involves integrating Artificial Intelligence for enhanced anomaly detection and predictive capabilities.\n","permalink":"http://localhost:1313/Akhil_Portfolio/projects/satellite/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThis \u003cstrong\u003eReal-Time Satellite Tracking and Trajectory Analysis\u003c/strong\u003e project is a comprehensive system designed to monitor the \u003cstrong\u003eInternational Space Station (ISS)\u003c/strong\u003e in real time, visualize its trajectory, and analyze its past and future paths. The project offers interactive visualizations, anomaly detection capabilities, and insights into the ISS\u0026rsquo;s movement across Earth.\u003c/p\u003e\n\u003ch3 id=\"key-features\"\u003e\u003cstrong\u003eKey Features\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eReal-Time Tracking:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eProvides the exact location of the ISS in real time, letting users trace its current position over the Earth.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTrajectory Visualization:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eDisplays the ISS\u0026rsquo;s recent path and predicts future trajectories with stunning Earth visualizations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDay \u0026amp; Night Views:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eIncorporates real-time \u003cstrong\u003eday-night boundary visualization\u003c/strong\u003e, syncing with Earth\u0026rsquo;s rotation.\u003c/li\u003e\n\u003cli\u003eOffers an understanding of how astronauts experience 16 sunrises and sunsets daily.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAnomaly Detection:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eAnalyzes the ISS\u0026rsquo;s trajectory to predict and highlight any anomalies in its path.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInteractive Exploration:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eAllows users to explore the ISS\u0026rsquo;s majestic journey, fostering curiosity about space exploration.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"working-process\"\u003e\u003cstrong\u003eWorking Process\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Acquisition:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eRetrieves real-time satellite data using \u003cstrong\u003eSkyfield API\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Processing:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eProcesses orbital elements to calculate and predict the ISS\u0026rsquo;s position and trajectory.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVisualization:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUtilizes \u003cstrong\u003eCartopy\u003c/strong\u003e for rendering Earth\u0026rsquo;s maps and the ISS\u0026rsquo;s path.\u003c/li\u003e\n\u003cli\u003eOverlays the ISS\u0026rsquo;s position, trajectory, and day-night boundaries.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAnomaly Analysis:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eImplements trajectory anomaly detection using historical data and predictions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"innovative-features\"\u003e\u003cstrong\u003eInnovative Features\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDynamic Earth Visualizations:\u003c/strong\u003e Realistic views of Earth with synchronized day-night boundaries.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHistorical Trajectory Analysis:\u003c/strong\u003e Displays the ISS\u0026rsquo;s past movements for a comprehensive context.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFuture Trajectory Predictions:\u003c/strong\u003e Predicts the ISS\u0026rsquo;s upcoming path with precision.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"tech-stack\"\u003e\u003cstrong\u003eTech Stack\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProgramming Language:\u003c/strong\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLibraries and Frameworks:\u003c/strong\u003e Skyfield, Cartopy, Matplotlib\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVisualization Tools:\u003c/strong\u003e Real-time map rendering, orbital path overlays\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTechniques:\u003c/strong\u003e Trajectory Prediction, Anomaly Detection, Data Visualization\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"applications\"\u003e\u003cstrong\u003eApplications\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSpace Exploration:\u003c/strong\u003e Real-time insights into satellite movements for enthusiasts and researchers.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEducational Tool:\u003c/strong\u003e Simplifies the complexities of orbital mechanics for learners.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAstronomy Integration:\u003c/strong\u003e Supports sky-gazing and satellite spotting for astronomy lovers.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"impact\"\u003e\u003cstrong\u003eImpact\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eThis project celebrates human curiosity and innovation by demystifying the ISS\u0026rsquo;s journey. By making satellite tracking accessible and interactive, it bridges the gap between technical data and public understanding. The next step involves integrating \u003cstrong\u003eArtificial Intelligence\u003c/strong\u003e for enhanced anomaly detection and predictive capabilities.\u003c/p\u003e","title":"Real-Time Satellite Tracking and Trajectory Analysis"},{"content":"Get in Touch Feel free to reach out to me through the following channels:\n📞 Phone: (+49 15163661969) 📧 Email: Email 🔗 LinkedIn: linkedin Looking forward to connecting with you! 🚀\n","permalink":"http://localhost:1313/Akhil_Portfolio/contact/","summary":"\u003ch1 id=\"get-in-touch\"\u003eGet in Touch\u003c/h1\u003e\n\u003cp\u003eFeel free to reach out to me through the following channels:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e📞 \u003cstrong\u003ePhone\u003c/strong\u003e:  (+49 15163661969)\u003c/li\u003e\n\u003cli\u003e📧 \u003cstrong\u003eEmail\u003c/strong\u003e: \u003ca href=\"mailto:rakhilsai95@gmail.com\"\u003eEmail\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e🔗 \u003cstrong\u003eLinkedIn\u003c/strong\u003e: \u003ca href=\"https://www.linkedin.com/in/akhil-sai-ravi-kumar-287830b5/\"\u003elinkedin\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLooking forward to connecting with you! 🚀\u003c/p\u003e","title":"Contact"},{"content":"Description The Quadcopter Live Aerial Video Surveillance System is an advanced drone platform designed to provide a live aerial video feed for real-time monitoring and analysis. This system enables rapid deployment of aerial correspondence for critical operations like search and rescue, firefighting, law enforcement, military applications, and news reporting. With future-focused capabilities such as target tracking and video compression, the system is designed for scalability and integration into various industries.\nKey Features Live Aerial Video Feed: Captures and transmits real-time video from the drone to a remote controller or computer. Digital Video Processing: Converts captured video signals into digital format for analysis and storage. Remote and Voice Control: Operates the drone with remote control, with future expansions for voice control integration. Auto-Landing: Ensures safe and precise landing using flight control systems. Target Tracking and Video Compression (Future Expansion): Potential to integrate advanced algorithms for target identification and bandwidth-efficient video transmission. Components Quadcopter/Controller System: Frame: Durable structure to support all hardware components. DC Motors and Rotors: Provide propulsion and maneuverability. Power Supply: Battery-driven system for sustained operation. Flight Control Board: Manages stability and flight dynamics. Flight Control Software: Ensures smooth operation and maneuvering. RC Controller: Enables precise remote navigation. Video Transmission System: Camera: Captures high-resolution aerial footage. Transmitter and Receiver: Streams live video feed to the ground control station. Display: Visualizes the live feed in real-time on a monitor or computer. Digital Video Analysis System: Signal Processing: Converts raw camera signals into a digital format. Video Output: Displays the processed feed on connected devices. Applications Search and Rescue: Rapid deployment in disaster-hit areas to locate survivors and assess damage. Firefighting: Real-time aerial monitoring to track fire spread and aid firefighting strategies. Law Enforcement: Surveillance and monitoring for public safety and criminal investigations. Military: Tactical reconnaissance and monitoring in high-risk zones. News Reporting: Live aerial coverage of events, enabling faster reporting. Working Process Data Acquisition: Captures video feed via a mounted camera on the quadcopter. Data Transmission: Transfers the video signal to the receiver using a transmitter. Signal Conversion: Converts the received signal into a digital video format. Video Analysis: Analyzes and stores video data for further use, with future provisions for tracking and compression. Tech Stack Hardware: Quadcopter frame, DC motors, rotors, flight control system, and RC controller. Software: Digital signal processing, flight control software. Video Transmission: Camera and transmitter/receiver modules. Impact This project bridges the gap between traditional surveillance systems and modern drone technology. It provides a faster, cost-effective, and versatile alternative to conventional helicopters, making it an indispensable tool in emergency response and monitoring scenarios. The future integration of sentience, target tracking, and video compression opens avenues for more advanced applications.\n","permalink":"http://localhost:1313/Akhil_Portfolio/projects/drone/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eQuadcopter Live Aerial Video Surveillance System\u003c/strong\u003e is an advanced drone platform designed to provide a live aerial video feed for real-time monitoring and analysis. This system enables rapid deployment of aerial correspondence for critical operations like \u003cstrong\u003esearch and rescue, firefighting, law enforcement, military applications\u003c/strong\u003e, and \u003cstrong\u003enews reporting\u003c/strong\u003e. With future-focused capabilities such as \u003cstrong\u003etarget tracking\u003c/strong\u003e and \u003cstrong\u003evideo compression\u003c/strong\u003e, the system is designed for scalability and integration into various industries.\u003c/p\u003e","title":"Quadcopter Live Aerial Video Surveillance System"},{"content":"Description As a Software Developer and Research Engineer specializing inGenerative AI, AI, and Machine Learning at ASTRIAL GmbH, I have spearheaded the development of a co-pilot designed to analyze security alerts, reduce false positives, and prioritize alarms, streamlining workflows for less-experienced security personnel.\nMy responsibilities included extensive TTP data preprocessing, where I rephrased and shortened sentences to preserve their original meaning, enhancing model comprehension. Leveraging SecureBERT, I handled domain-specific terms to improve the detection and understanding of complex technical concepts in cybersecurity data. Additionally, I utilized GPT-3.5 to rephrase sentences, ensuring readability, diversity, and clarity while maintaining their core intent.\nTo enable intuitive threat analysis, I integrated preprocessed TTP data into Neo4j, visualizing TTPs and SubTTPs along with their relationships. Further, I incorporated Llama2 with the Neo4j GraphDatabase, empowering the co-pilot to guide users in navigating complex security datasets and identifying techniques and tactics effectively.\nMy expertise spans deep learning models such as BERT, GPT, T5, and OpenAI APIs, applied to tasks like text classification, summarization, and question-answering. I have fine-tuned LLMs (e.g., Llama2, GPT) for domain-specific applications, applied prompt engineering to optimize LLM outputs, and implemented cosine similarity and vector embeddings for tasks like Named Entity Recognition (NER) and semantic search.\nIn addition to my NLP expertise, I have delivered end-to-end image processing solutions, including data acquisition, preprocessing, model training, and deployment. I enhanced object detection and segmentation algorithms using tools like YoloV5, OpenCV, and SAM (Segment Anything Model) while optimizing models for edge device deployments with ONNX and OpenVINO.\nFurther accomplishments include designing sound detection algorithms using FFT and geo-localization techniques, applying advanced audio preprocessing for signal quality optimization, and deploying applications with Docker-based CI/CD pipelines for scalable, reliable delivery. My focus on model robustness and data optimization techniques has ensured consistent, high-quality outputs.\nI also contributed to team development by mentoring interns and master’s students, fostering technical excellence and collaborative learning. Through a combination of technical innovation and mentorship, I have driven impactful solutions in AI-powered cybersecurity, computer vision, and generative AI.\n","permalink":"http://localhost:1313/Akhil_Portfolio/experience/astrial/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs a \u003cstrong\u003eSoftware Developer and Research Engineer\u003c/strong\u003e specializing in\u003cstrong\u003eGenerative AI, AI, and Machine Learning\u003c/strong\u003e at \u003cstrong\u003eASTRIAL GmbH\u003c/strong\u003e, I have spearheaded the development of a \u003cstrong\u003eco-pilot\u003c/strong\u003e designed to analyze security alerts, reduce false positives, and prioritize alarms, streamlining workflows for less-experienced security personnel.\u003c/p\u003e\n\u003cp\u003eMy responsibilities included extensive TTP data preprocessing, where I rephrased and shortened sentences to preserve their original meaning, enhancing model comprehension. Leveraging \u003cstrong\u003eSecureBERT\u003c/strong\u003e, I handled domain-specific terms to improve the detection and understanding of complex technical concepts in \u003cstrong\u003ecybersecurity\u003c/strong\u003e data. Additionally, I utilized \u003cstrong\u003eGPT-3.5\u003c/strong\u003e to rephrase sentences, ensuring readability, diversity, and clarity while maintaining their core intent.\u003c/p\u003e","title":"Senior Software Developer and Research engineer"},{"content":"Description As a Python Developer specializing in Simulation Frameworks, UI Development, and Machine Learning, I contributed to building advanced systems for demand-driven environments and predictive modeling. My work focused on enhancing supply chain management tools, developing user-friendly interfaces, and creating machine learning models to improve forecasting and decision-making processes.\nI designed and implemented an improved User Interface (UI) for DDMRP (Demand-Driven Material Requirements Planning) and MRP (Material Requirements Planning) engines, optimizing usability and operational efficiency in supply chain systems. Leveraging Flask, I built a dynamic web application to display, edit, and manage input values for simulation engines, including WMAPE, DDMRP, and MRP, enabling real-time updates and streamlined data handling.\nTo standardize and accelerate development, I developed custom reusable User Controls in Flask, ensuring consistent and efficient design across multiple screens. I also architected key components such as the Data Access Layer (DAL) and Business Logic Layer (BLL), ensuring robust backend architecture and seamless data flow.\nIn the domain of security and authentication, I implemented secure user sign-in and sign-up mechanisms, integrating third-party authentication providers like Google and Microsoft to enhance application security. Additionally, I developed a WMAPE automatic error identification system, incorporating heatmap graphs to provide intuitive visual diagnostics for demand generator engines.\nI also contributed to machine learning model development, building models for time series data generation and future demand prediction. By utilizing algorithms such as Random Forest, Isolation Forest, and LSTM neural networks, I improved forecasting accuracy and reliability in dynamic demand environments.\nThrough performance optimization and troubleshooting, I ensured seamless operation and an enhanced user experience across all modules. My technical expertise in UI development, backend architecture, and machine learning enabled me to deliver impactful solutions for simulation frameworks and demand-driven applications.\n","permalink":"http://localhost:1313/Akhil_Portfolio/experience/camelot/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs a \u003cstrong\u003ePython Developer\u003c/strong\u003e specializing in Simulation \u003cstrong\u003eFrameworks, UI Development\u003c/strong\u003e, and \u003cstrong\u003eMachine Learning\u003c/strong\u003e, I contributed to building advanced systems for demand-driven environments and predictive modeling. My work focused on enhancing supply chain management tools, developing user-friendly interfaces, and creating machine learning models to improve forecasting and decision-making processes.\u003c/p\u003e\n\u003cp\u003eI designed and implemented an improved User Interface (\u003cstrong\u003eUI\u003c/strong\u003e) for \u003cstrong\u003eDDMRP\u003c/strong\u003e (Demand-Driven Material Requirements Planning) and \u003cstrong\u003eMRP\u003c/strong\u003e (Material Requirements Planning) engines, optimizing usability and operational efficiency in supply chain systems. Leveraging \u003cstrong\u003eFlask\u003c/strong\u003e, I built a dynamic web application to display, edit, and manage input values for simulation engines, including \u003cstrong\u003eWMAPE, DDMRP, and MRP, enabling real-time updates\u003c/strong\u003e and \u003cstrong\u003estreamlined data handling\u003c/strong\u003e.\u003c/p\u003e","title":"Python Developer (Intern)"},{"content":"Description As a Software Design and Developer Engineer at Tech Mahindra LTD, I worked extensively on designing, developing, and deploying software solutions leveraging technologies like Python, AWS, .NET, HTML, CSS, JavaScript, and Microsoft Azure Cloud. My role involved automation, responsive web design, cloud infrastructure management, and enhancing application performance for seamless operations.\nI automated Azure resource creation using PowerShell scripts, managing resource groups, web applications, storage blobs/tables, and firewall rules to optimize cloud deployment processes. Additionally, I developed responsive web designs (RWD) using Media Queries, Bootstrap, HTML, CSS, and JavaScript, ensuring cross-device compatibility and delivering consistent user experiences across platforms.\nIn application development and deployment, I configured and managed IIS Web Servers for hosting and collaborated in system specification meetings to align development with user requirements. I designed and implemented CI/CD pipelines using AWS CodePipeline, CodeCommit, and CodeDeploy, streamlining continuous integration and delivery for efficient software deployment.\nMy expertise extended to cloud infrastructure management, where I deployed and maintained Azure IaaS virtual machines (VMs), PaaS role instances, and AWS Elastic Beanstalk environments, ensuring scalable and reliable infrastructure. I also managed version control systems using GIT, GitHub, and TFS, employing structured branching strategies to maintain codebase integrity and foster collaboration.\nAs part of technical troubleshooting, I diagnosed and resolved application performance issues, verified data integrity, and identified code defects. I implemented SSO policies and federated identities, managing dynamic group memberships and user access policies across multiple SaaS applications to enhance security and identity management.\nFurthermore, I collaborated on database functionalities, data analytics, and reporting requirements, ensuring alignment with organizational objectives. My comprehensive skill set in software development, cloud automation, and infrastructure management enabled me to deliver high-impact solutions for enterprise-grade applications.\n","permalink":"http://localhost:1313/Akhil_Portfolio/experience/tech-mahindra-ltd/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs a Software Design and Developer Engineer at \u003cstrong\u003eTech Mahindra LTD\u003c/strong\u003e, I worked extensively on designing, developing, and deploying software solutions leveraging technologies like Python, AWS, .NET, HTML, CSS, JavaScript, and Microsoft Azure Cloud. My role involved \u003cstrong\u003eautomation, responsive web design, cloud infrastructure management\u003c/strong\u003e, and \u003cstrong\u003eenhancing application performance\u003c/strong\u003e for seamless operations.\u003c/p\u003e\n\u003cp\u003eI automated Azure resource creation using PowerShell scripts, managing resource groups, web applications, storage blobs/tables, and firewall rules to optimize cloud deployment processes. Additionally, I developed responsive web designs (\u003cstrong\u003eRWD\u003c/strong\u003e) using \u003cstrong\u003eMedia Queries, Bootstrap, HTML, CSS\u003c/strong\u003e, and \u003cstrong\u003eJavaScript\u003c/strong\u003e, ensuring cross-device compatibility and delivering consistent user experiences across platforms.\u003c/p\u003e","title":"Software Design and Developer Engineer"},{"content":"Description As a Junior Software Developer (Intern) specializing in AI, ML, and Robotics, I played a pivotal role in designing and developing a prototype smart rover named as MOWME, leveraging AI, ML, and DL technologies to enable autonomous operations. The rover was equipped with SLAM (Simultaneous Localization and Mapping) capabilities and utilized CNN networks for task identification and classification. I integrated advanced features such as object detection, obstacle avoidance, GPS waypoint generation, and self-testing mechanisms, achieving seamless autonomous navigation.\nTo ensure high-quality datasets, I built automated data preprocessing pipelines, incorporating steps like Normalization, Standardization, Resizing, Denoising, Segmentation, and Morphology, as well as JSON creation and Data Pickling for efficient data handling. These pipelines enabled consistent and reliable input data for machine learning workflows.\nIn the domain of Machine Learning and Model Optimization, I implemented Regression, Classification, and Ensemble algorithms, troubleshooting bottlenecks to enhance model accuracy and reduce inference latency. Optimized models were deployed on edge devices such as Raspberry Pi 3, Jetson Nano, and AWS EC2 GPU instances, leveraging GPU acceleration with tools like CUDA and cuDNN for efficient performance.\nAdditionally, I integrated IoT devices like TCG4 sensors and Beacon devices to collect metrics (e.g., acceleration, pressure, temperature, humidity) and established cloud communication pipelines with AWS. I worked with advanced communication protocols including SOMEIP, VSOMEIP, and CAN to ensure robust and stable device connectivity.\nMy contributions also included designing and developing a web-based AI model development platform, enabling users to build image-based AI models for object detection, classification, and recognition without requiring programming expertise.\nThrough effective CI/CD pipeline management and version control systems like GitHub and GitLab, I ensured streamlined workflows, reliable deployments, and system scalability. My work combined technical expertise and innovation to deliver a versatile smart rover and empower users with accessible AI-driven solutions.\n","permalink":"http://localhost:1313/Akhil_Portfolio/experience/avl-software-and-functions-gmbh/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eAs a \u003cstrong\u003eJunior Software Developer\u003c/strong\u003e (Intern) specializing in \u003cstrong\u003eAI, ML\u003c/strong\u003e, and \u003cstrong\u003eRobotics\u003c/strong\u003e, I played a pivotal role in designing and developing a prototype \u003cstrong\u003esmart rover\u003c/strong\u003e named as \u003cstrong\u003eMOWME\u003c/strong\u003e, leveraging AI, ML, and DL technologies to enable autonomous operations. The rover was equipped with \u003cstrong\u003eSLAM\u003c/strong\u003e (Simultaneous Localization and Mapping) capabilities and utilized CNN networks for task identification and classification. I integrated advanced features such as \u003cstrong\u003eobject detection\u003c/strong\u003e, \u003cstrong\u003eobstacle avoidance\u003c/strong\u003e, \u003cstrong\u003eGPS\u003c/strong\u003e waypoint generation, and self-testing mechanisms, achieving seamless autonomous navigation.\u003c/p\u003e","title":"Junior Software Developer (Intern)"}]